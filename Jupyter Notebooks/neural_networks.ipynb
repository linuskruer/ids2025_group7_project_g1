{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316aaa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.18\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow==2.18) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (1.76.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow==2.18)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (3.15.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow==2.18)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow==2.18) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: ml-dtypes, tensorboard, tensorflow\n",
      "\n",
      "  Attempting uninstall: ml-dtypes\n",
      "\n",
      "    Found existing installation: ml_dtypes 0.5.4\n",
      "\n",
      "    Uninstalling ml_dtypes-0.5.4:\n",
      "\n",
      "      Successfully uninstalled ml_dtypes-0.5.4\n",
      "\n",
      "  Attempting uninstall: tensorboard\n",
      "\n",
      "    Found existing installation: tensorboard 2.20.0\n",
      "\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "    Uninstalling tensorboard-2.20.0:\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "      Successfully uninstalled tensorboard-2.20.0\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "  Attempting uninstall: tensorflow\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "    Found existing installation: tensorflow 2.20.0\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "    Uninstalling tensorflow-2.20.0:\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "      Successfully uninstalled tensorflow-2.20.0\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   ---------------------------------------- 3/3 [tensorflow]\n",
      "\n",
      "Successfully installed ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0\n",
      "Requirement already satisfied: pip in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (25.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.18.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.9.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.76.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.15.1)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.4-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (4.2.2)\n",
      "Requirement already satisfied: promise in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (22.0.0)\n",
      "Requirement already satisfied: simple_parsing in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: toml in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: einops in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.10.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.23.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dm-tree->tensorflow_datasets) (25.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.72.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hwind\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->tensorflow_datasets) (0.4.6)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "Using cached ml_dtypes-0.5.4-cp311-cp311-win_amd64.whl (210 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: ml_dtypes, tensorboard, tensorflow\n",
      "\n",
      "  Attempting uninstall: ml_dtypes\n",
      "\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "\n",
      "  Attempting uninstall: tensorboard\n",
      "\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "  Attempting uninstall: tensorflow\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "   ------------- -------------------------- 1/3 [tensorboard]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   -------------------------- ------------- 2/3 [tensorflow]\n",
      "   ---------------------------------------- 3/3 [tensorflow]\n",
      "\n",
      "Successfully installed ml_dtypes-0.5.4 tensorboard-2.20.0 tensorflow-2.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.20.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.18\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ef2f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwind\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#code for feature engineering\n",
    "def featurize_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(2050)\n",
    "\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    tpsa = Descriptors.TPSA(mol)\n",
    "\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    fp = np.array(fp)\n",
    "\n",
    "    return np.concatenate([[mw, logp, hbd, hba, tpsa], fp])\n",
    "train = pd.read_csv(\"/Users/hwind/Downloads/Python/Master/3_Semester/Data Science/Project/ids2025_group7_project_g1/Data/train.csv\")\n",
    "test = pd.read_csv(\"/Users/hwind/Downloads/Python/Master/3_Semester/Data Science/Project/ids2025_group7_project_g1/Data/test.csv\")\n",
    "\n",
    "X_train = np.vstack(train[\"SMILES\"].apply(featurize_smiles)).astype('float32')\n",
    "X_test = np.vstack(test[\"SMILES\"].apply(featurize_smiles)).astype('float32')\n",
    "y_train = train[\"Tm\"].values.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b7cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  Loss: 70523.1641\n",
      "Epoch 02  Loss: 24247.8750\n",
      "Epoch 03  Loss: 9856.8311\n",
      "Epoch 04  Loss: 8316.0889\n",
      "Epoch 05  Loss: 7692.0044\n",
      "Epoch 06  Loss: 7235.2231\n",
      "Epoch 07  Loss: 6825.7871\n",
      "Epoch 08  Loss: 6354.2930\n",
      "Epoch 09  Loss: 5923.7734\n",
      "Epoch 10  Loss: 5477.4316\n",
      "Epoch 11  Loss: 5074.1577\n",
      "Epoch 12  Loss: 4667.1147\n",
      "Epoch 13  Loss: 4386.2671\n",
      "Epoch 14  Loss: 4041.6868\n",
      "Epoch 15  Loss: 3838.8848\n",
      "Epoch 16  Loss: 3585.7917\n",
      "Epoch 17  Loss: 3499.6199\n",
      "Epoch 18  Loss: 3311.0105\n",
      "Epoch 19  Loss: 3050.7771\n",
      "Epoch 20  Loss: 2930.0435\n",
      "\n",
      "example_prediction: [333.5938  384.3297  181.126   197.14261 244.22209]\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "batch_size = 128\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train.astype('float32'), y_train.astype('float32')))\n",
    "ds_train = ds_train.shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "initializer = tf.initializers.GlorotUniform()\n",
    "\n",
    "weights = {\n",
    "    \"h1\": tf.Variable(initializer([num_features, n_hidden_1], dtype=tf.float32)),\n",
    "    \"h2\": tf.Variable(initializer([n_hidden_1, n_hidden_2], dtype=tf.float32)),\n",
    "    \"out\": tf.Variable(initializer([n_hidden_2, 1], dtype=tf.float32))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"b1\": tf.Variable(tf.zeros([n_hidden_1], dtype=tf.float32)),\n",
    "    \"b2\": tf.Variable(tf.zeros([n_hidden_2], dtype=tf.float32)),\n",
    "    \"out\": tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "}\n",
    "\n",
    "#Model\n",
    "def neural_net(x):\n",
    "    layer1 = tf.nn.relu(tf.matmul(x, weights[\"h1\"]) + biases[\"b1\"])\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, weights[\"h2\"]) + biases[\"b2\"])\n",
    "    out = tf.matmul(layer2, weights[\"out\"]) + biases[\"out\"]  # LINEAR OUTPUT\n",
    "    return out  # shape: (batch, 1)\n",
    "\n",
    "#Loss + Optimizer\n",
    "def mse_loss(y_pred, y_true):\n",
    "    y_true = tf.reshape(y_true, (-1, 1))\n",
    "    return tf.reduce_mean((y_true - y_pred) ** 2)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "#print(\"x dtype:\", bx.dtype)\n",
    "#print(\"weights dtype:\", weights[\"h1\"].dtype)\n",
    "\n",
    "#Training\n",
    "def run_optimization(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = neural_net(x)\n",
    "        loss = mse_loss(pred, y)\n",
    "\n",
    "    trainable_vars = list(weights.values()) + list(biases.values())\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    return loss\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    batch_losses = []\n",
    "    for bx, by in ds_train:\n",
    "        loss = run_optimization(bx, by)\n",
    "        batch_losses.append(loss.numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  Loss: {np.mean(batch_losses):.4f}\")\n",
    "\n",
    "# prediction\n",
    "pred_test = neural_net(X_test).numpy().flatten()\n",
    "\n",
    "print(\"\\nexample_prediction:\", pred_test[:5])\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": pred_test\n",
    "})\n",
    "\n",
    "#submission.to_csv(\"Submissions/predictions_NN.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6eec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 79415.0078\n",
      "Epoch 2/300\n",
      "\u001b[1m 4/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 68616.3867 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwind\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 66682.3281\n",
      "Epoch 3/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 45194.4844\n",
      "Epoch 4/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 23276.1035\n",
      "Epoch 5/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9299.9072 \n",
      "Epoch 6/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6281.6230\n",
      "Epoch 7/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4951.7944\n",
      "Epoch 8/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3439.5146\n",
      "Epoch 9/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2936.0452\n",
      "Epoch 10/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2540.1931\n",
      "Epoch 11/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1802.6133\n",
      "Epoch 12/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1908.3459\n",
      "Epoch 13/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2053.9492\n",
      "Epoch 14/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1732.8793\n",
      "Epoch 15/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1620.0907\n",
      "Epoch 16/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1843.9722\n",
      "Epoch 17/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1628.7618\n",
      "Epoch 18/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1997.4167\n",
      "Epoch 19/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1406.9738\n",
      "Epoch 20/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1515.9501\n",
      "Epoch 21/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1570.1208\n",
      "Epoch 22/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1333.8533\n",
      "Epoch 23/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1175.8247\n",
      "Epoch 24/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1105.1141\n",
      "Epoch 25/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1269.8964\n",
      "Epoch 26/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1486.2462\n",
      "Epoch 27/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1640.2740\n",
      "Epoch 28/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1260.8081\n",
      "Epoch 29/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1387.2900\n",
      "Epoch 30/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1309.6359\n",
      "Epoch 31/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1393.1642\n",
      "Epoch 32/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1403.5813\n",
      "Epoch 33/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1463.3390\n",
      "Epoch 34/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1432.2496\n",
      "Epoch 35/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1514.7074\n",
      "Epoch 36/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 928.2466\n",
      "Epoch 37/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1146.6621\n",
      "Epoch 38/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1023.1755\n",
      "Epoch 39/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1267.0220\n",
      "Epoch 40/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 953.2664\n",
      "Epoch 41/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1186.1482\n",
      "Epoch 42/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1320.3134\n",
      "Epoch 43/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1482.1615\n",
      "Epoch 44/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1501.8345\n",
      "Epoch 45/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1047.2795\n",
      "Epoch 46/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 829.7929\n",
      "Epoch 47/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1222.5841\n",
      "Epoch 48/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1100.7476\n",
      "Epoch 49/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 969.2899\n",
      "Epoch 50/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 870.2845\n",
      "Epoch 51/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 878.6111\n",
      "Epoch 52/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 983.7547\n",
      "Epoch 53/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 788.1550\n",
      "Epoch 54/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 872.1642\n",
      "Epoch 55/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 854.2122\n",
      "Epoch 56/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1087.5258\n",
      "Epoch 57/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 868.8399\n",
      "Epoch 58/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 947.0898\n",
      "Epoch 59/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1043.0000\n",
      "Epoch 60/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 875.9116\n",
      "Epoch 61/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 904.4778\n",
      "Epoch 62/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 873.9678\n",
      "Epoch 63/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 868.5150\n",
      "Epoch 64/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 693.3591\n",
      "Epoch 65/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 984.0455 \n",
      "Epoch 66/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 718.2313\n",
      "Epoch 67/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 648.8529\n",
      "Epoch 68/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 695.9931\n",
      "Epoch 69/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 677.1391\n",
      "Epoch 70/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 706.0236\n",
      "Epoch 71/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 606.0795\n",
      "Epoch 72/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 653.0735\n",
      "Epoch 73/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 662.6481\n",
      "Epoch 74/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 663.2889\n",
      "Epoch 75/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 592.9319\n",
      "Epoch 76/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 760.3687\n",
      "Epoch 77/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 596.1837\n",
      "Epoch 78/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 570.9691\n",
      "Epoch 79/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 590.8877\n",
      "Epoch 80/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 579.5850\n",
      "Epoch 81/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 530.8397\n",
      "Epoch 82/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 608.1327\n",
      "Epoch 83/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 487.0221\n",
      "Epoch 84/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 421.0506\n",
      "Epoch 85/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 492.5616\n",
      "Epoch 86/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 554.1307\n",
      "Epoch 87/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 622.8969\n",
      "Epoch 88/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 458.8536\n",
      "Epoch 89/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 493.9505\n",
      "Epoch 90/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 509.3649\n",
      "Epoch 91/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 381.2134\n",
      "Epoch 92/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 380.9995\n",
      "Epoch 93/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 462.7984\n",
      "Epoch 94/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 555.7413\n",
      "Epoch 95/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 652.2715\n",
      "Epoch 96/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 640.4091\n",
      "Epoch 97/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 440.2655\n",
      "Epoch 98/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 363.5055\n",
      "Epoch 99/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 453.3502\n",
      "Epoch 100/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 382.3911\n",
      "Epoch 101/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 336.7244\n",
      "Epoch 102/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 331.1364\n",
      "Epoch 103/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 364.6850\n",
      "Epoch 104/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 334.1130\n",
      "Epoch 105/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 376.4398\n",
      "Epoch 106/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 289.0227\n",
      "Epoch 107/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 354.0104\n",
      "Epoch 108/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 264.8738\n",
      "Epoch 109/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 449.1548\n",
      "Epoch 110/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 406.6655\n",
      "Epoch 111/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 342.9442\n",
      "Epoch 112/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 413.2202\n",
      "Epoch 113/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 363.1290\n",
      "Epoch 114/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 417.4416\n",
      "Epoch 115/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 331.8429\n",
      "Epoch 116/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 348.1709\n",
      "Epoch 117/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 408.6388\n",
      "Epoch 118/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 305.6672\n",
      "Epoch 119/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 317.6255\n",
      "Epoch 120/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 304.4224\n",
      "Epoch 121/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 337.9619\n",
      "Epoch 122/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 382.4480\n",
      "Epoch 123/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 312.6351\n",
      "Epoch 124/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 265.4678\n",
      "Epoch 125/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 304.8895\n",
      "Epoch 126/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 377.7925\n",
      "Epoch 127/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 351.3453\n",
      "Epoch 128/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 264.0953\n",
      "Epoch 129/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 280.5840\n",
      "Epoch 130/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 291.1953\n",
      "Epoch 131/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 281.0060\n",
      "Epoch 132/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 323.5223\n",
      "Epoch 133/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 284.8235\n",
      "Epoch 134/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 355.1565\n",
      "Epoch 135/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 424.6218\n",
      "Epoch 136/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 369.1250\n",
      "Epoch 137/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 374.3964\n",
      "Epoch 138/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 323.9659\n",
      "Epoch 139/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 327.9731\n",
      "Epoch 140/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 348.0233\n",
      "Epoch 141/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 340.6690\n",
      "Epoch 142/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 334.8115\n",
      "Epoch 143/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 309.6153\n",
      "Epoch 144/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 263.2133\n",
      "Epoch 145/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 279.4667\n",
      "Epoch 146/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 300.6950\n",
      "Epoch 147/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 225.0496\n",
      "Epoch 148/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 252.0734\n",
      "Epoch 149/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 316.7006\n",
      "Epoch 150/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 272.1210\n",
      "Epoch 151/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 318.9647\n",
      "Epoch 152/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 312.3935\n",
      "Epoch 153/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 230.6874\n",
      "Epoch 154/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 244.3934\n",
      "Epoch 155/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 266.2224\n",
      "Epoch 156/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 247.4456\n",
      "Epoch 157/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 242.7949\n",
      "Epoch 158/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 264.7454\n",
      "Epoch 159/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 203.8177\n",
      "Epoch 160/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 236.5526\n",
      "Epoch 161/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 211.8204\n",
      "Epoch 162/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 290.1317\n",
      "Epoch 163/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 236.3659\n",
      "Epoch 164/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 241.0352\n",
      "Epoch 165/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 222.5258\n",
      "Epoch 166/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 224.4183\n",
      "Epoch 167/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 280.7301\n",
      "Epoch 168/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 231.4210\n",
      "Epoch 169/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 262.6024\n",
      "Epoch 170/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 267.9346\n",
      "Epoch 171/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 297.7036\n",
      "Epoch 172/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 376.9012\n",
      "Epoch 173/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 268.0832\n",
      "Epoch 174/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 243.6375\n",
      "Epoch 175/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 310.4879\n",
      "Epoch 176/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 247.1422\n",
      "Epoch 177/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 244.6043\n",
      "Epoch 178/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 219.4654\n",
      "Epoch 179/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 234.6245\n",
      "Epoch 180/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 177.3995\n",
      "Epoch 181/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 199.4137\n",
      "Epoch 182/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 232.4776\n",
      "Epoch 183/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 267.7559\n",
      "Epoch 184/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 281.2296\n",
      "Epoch 185/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 240.0774\n",
      "Epoch 186/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 287.3114\n",
      "Epoch 187/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 316.4961\n",
      "Epoch 188/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 237.4647\n",
      "Epoch 189/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 265.5869\n",
      "Epoch 190/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 211.1967\n",
      "Epoch 191/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 162.4407\n",
      "Epoch 192/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 202.6176\n",
      "Epoch 193/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 236.2554\n",
      "Epoch 194/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 297.7790\n",
      "Epoch 195/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 242.8136\n",
      "Epoch 196/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 259.0146\n",
      "Epoch 197/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 242.8991\n",
      "Epoch 198/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 216.9068\n",
      "Epoch 199/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 241.4133\n",
      "Epoch 200/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 282.6020\n",
      "Epoch 201/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 257.0607\n",
      "Epoch 202/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 201.0270\n",
      "Epoch 203/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 286.5547\n",
      "Epoch 204/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 212.6008\n",
      "Epoch 205/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 251.4152\n",
      "Epoch 206/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 318.8079\n",
      "Epoch 207/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 288.1809\n",
      "Epoch 208/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 243.1466\n",
      "Epoch 209/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 212.5710\n",
      "Epoch 210/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 261.7757\n",
      "Epoch 211/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 225.9375\n",
      "Epoch 212/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 211.6399\n",
      "Epoch 213/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 253.2375\n",
      "Epoch 214/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 210.4948\n",
      "Epoch 215/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 228.7249\n",
      "Epoch 216/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 248.7234\n",
      "Epoch 217/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 243.0927\n",
      "Epoch 218/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 231.6584\n",
      "Epoch 219/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 246.7422\n",
      "Epoch 220/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 278.8917\n",
      "Epoch 221/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 219.1579\n",
      "Epoch 222/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 219.9536\n",
      "Epoch 223/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 200.7081\n",
      "Epoch 224/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 215.1116\n",
      "Epoch 225/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 224.1311\n",
      "Epoch 226/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 242.5639\n",
      "Epoch 227/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 220.9297\n",
      "Epoch 228/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 232.3912\n",
      "Epoch 229/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 235.2338\n",
      "Epoch 230/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 210.5077\n",
      "Epoch 231/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 220.0479\n",
      "Epoch 232/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 239.1944\n",
      "Epoch 233/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 210.0669\n",
      "Epoch 234/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 202.0521\n",
      "Epoch 235/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 222.8251\n",
      "Epoch 236/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 227.3696\n",
      "Epoch 237/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 213.5170\n",
      "Epoch 238/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 275.9248\n",
      "Epoch 239/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 268.1363\n",
      "Epoch 240/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 337.8110\n",
      "Epoch 241/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 256.6231\n",
      "Epoch 242/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 267.6416\n",
      "Epoch 243/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 217.2028\n",
      "Epoch 244/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 265.0381\n",
      "Epoch 245/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 233.8997\n",
      "Epoch 246/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 243.6249\n",
      "Epoch 247/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 253.1933\n",
      "Epoch 248/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 230.7439\n",
      "Epoch 249/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 210.8939\n",
      "Epoch 250/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 220.7971\n",
      "Epoch 251/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 204.8622\n",
      "Epoch 252/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 217.3950\n",
      "Epoch 253/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 203.0143\n",
      "Epoch 254/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 243.1986\n",
      "Epoch 255/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 231.4217\n",
      "Epoch 256/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 211.3478\n",
      "Epoch 257/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 196.8995\n",
      "Epoch 258/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 307.9212\n",
      "Epoch 259/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 223.7629\n",
      "Epoch 260/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 187.8011\n",
      "Epoch 261/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 200.2177\n",
      "Epoch 262/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 195.7021\n",
      "Epoch 263/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 175.7498\n",
      "Epoch 264/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 179.5475\n",
      "Epoch 265/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 159.6530\n",
      "Epoch 266/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 176.9594\n",
      "Epoch 267/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 210.7120\n",
      "Epoch 268/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 202.9839\n",
      "Epoch 269/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 239.3067\n",
      "Epoch 270/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 186.1632\n",
      "Epoch 271/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 182.2141\n",
      "Epoch 272/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 231.0849\n",
      "Epoch 273/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 229.6353\n",
      "Epoch 274/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 208.9406\n",
      "Epoch 275/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 194.6934\n",
      "Epoch 276/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 191.0590\n",
      "Epoch 277/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 170.6978\n",
      "Epoch 278/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 193.5161\n",
      "Epoch 279/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 192.4915\n",
      "Epoch 280/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 162.0901\n",
      "Epoch 281/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 193.1116\n",
      "Epoch 282/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 169.8866\n",
      "Epoch 283/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 149.6004\n",
      "Epoch 284/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 190.4636\n",
      "Epoch 285/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 134.3323\n",
      "Epoch 286/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 178.6541\n",
      "Epoch 287/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 214.8287\n",
      "Epoch 288/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 187.2583\n",
      "Epoch 289/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 183.3946\n",
      "Epoch 290/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 165.8349\n",
      "Epoch 291/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 236.2507\n",
      "Epoch 292/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 227.2620\n",
      "Epoch 293/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 256.4751\n",
      "Epoch 294/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 258.4147\n",
      "Epoch 295/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 238.3747\n",
      "Epoch 296/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 198.5660\n",
      "Epoch 297/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 194.7376\n",
      "Epoch 298/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 192.9145\n",
      "Epoch 299/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 215.2433\n",
      "Epoch 300/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 202.1156\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>344.937775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1146</td>\n",
       "      <td>256.665710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>208.942673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2279</td>\n",
       "      <td>220.955521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342</td>\n",
       "      <td>250.004120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2663</td>\n",
       "      <td>276.214050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>624</td>\n",
       "      <td>220.769348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2655</td>\n",
       "      <td>194.528580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2089</td>\n",
       "      <td>255.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1065</td>\n",
       "      <td>259.484497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>666 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          Tm\n",
       "0    1022  344.937775\n",
       "1    1146  256.665710\n",
       "2      79  208.942673\n",
       "3    2279  220.955521\n",
       "4    1342  250.004120\n",
       "..    ...         ...\n",
       "661  2663  276.214050\n",
       "662   624  220.769348\n",
       "663  2655  194.528580\n",
       "664  2089  255.882507\n",
       "665  1065  259.484497\n",
       "\n",
       "[666 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best try\n",
    "#TensorFlow Dataset\n",
    "batch_size = 128\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train.astype(\"float32\")))\n",
    "ds_train = ds_train.shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#better model\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)  # linear output\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "#training\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=300,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": pred_test\n",
    "})#.to_csv(\"Submissions/predictions_NN_improved.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8259c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Modell mit Seed 0\n",
      "Epoch 1/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - loss: 71650.3438 - val_loss: 27928.5820\n",
      "Epoch 2/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 21966.4473 - val_loss: 8225.7783\n",
      "Epoch 3/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 4305.8760 - val_loss: 9724.8135\n",
      "Epoch 4/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 3081.9299 - val_loss: 6910.1196\n",
      "Epoch 5/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2807.1604 - val_loss: 10249.9863\n",
      "Epoch 6/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2316.7400 - val_loss: 16139.3545\n",
      "Epoch 7/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2208.0989 - val_loss: 14996.2520\n",
      "Epoch 8/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1892.7788 - val_loss: 12685.8613\n",
      "Epoch 9/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1779.5704 - val_loss: 14434.3643\n",
      "Epoch 10/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1740.8087 - val_loss: 14330.3457\n",
      "Epoch 11/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1655.9329 - val_loss: 11460.5918\n",
      "Epoch 12/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1548.2257 - val_loss: 16383.7305\n",
      "Epoch 13/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1670.3558 - val_loss: 16503.5898\n",
      "Epoch 14/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1513.6516 - val_loss: 14949.3535\n",
      "Epoch 15/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1525.5884 - val_loss: 10585.9238\n",
      "Epoch 16/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1227.8025 - val_loss: 13834.3936\n",
      "Epoch 17/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1619.3506 - val_loss: 8735.1650\n",
      "Epoch 18/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1177.7126 - val_loss: 10563.1670\n",
      "Epoch 19/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1207.1736 - val_loss: 8000.0703\n",
      "Epoch 20/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1217.3192 - val_loss: 6657.5615\n",
      "Epoch 21/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1102.5116 - val_loss: 10811.7334\n",
      "Epoch 22/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 968.2868 - val_loss: 10160.0176\n",
      "Epoch 23/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 946.4876 - val_loss: 6388.5620\n",
      "Epoch 24/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1211.0244 - val_loss: 9531.2754\n",
      "Epoch 25/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1024.6974 - val_loss: 9654.7354\n",
      "Epoch 26/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 962.3698 - val_loss: 9958.4453\n",
      "Epoch 27/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1302.7251 - val_loss: 12361.0273\n",
      "Epoch 28/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 900.3315 - val_loss: 9683.9209\n",
      "Epoch 29/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 881.5349 - val_loss: 9255.0156\n",
      "Epoch 30/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 827.0383 - val_loss: 9117.5879\n",
      "Epoch 31/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 954.1387 - val_loss: 10303.1328\n",
      "Epoch 32/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 924.8683 - val_loss: 8250.4004\n",
      "Epoch 33/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 795.2766 - val_loss: 8339.5576\n",
      "Epoch 34/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 825.5126 - val_loss: 7064.5488\n",
      "Epoch 35/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 798.7446 - val_loss: 8328.1318\n",
      "Epoch 36/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 952.6053 - val_loss: 8195.4189\n",
      "Epoch 37/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 882.2231 - val_loss: 8979.4785\n",
      "Epoch 38/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 771.7944 - val_loss: 9032.6006\n",
      "Epoch 39/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 881.2338 - val_loss: 7081.6431\n",
      "Epoch 40/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 820.3516 - val_loss: 9502.7637\n",
      "Epoch 41/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 1037.7140 - val_loss: 7523.4731\n",
      "Epoch 42/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 677.0859 - val_loss: 6731.2729\n",
      "Epoch 43/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 825.8995 - val_loss: 6499.8809\n",
      "Epoch 44/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 813.1812 - val_loss: 9010.0312\n",
      "Epoch 45/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 863.6445 - val_loss: 7351.7896\n",
      "Epoch 46/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 862.1207 - val_loss: 7226.0854\n",
      "Epoch 47/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 732.6974 - val_loss: 5461.9277\n",
      "Epoch 48/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 770.3306 - val_loss: 6443.2197\n",
      "Epoch 49/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 755.5237 - val_loss: 4934.7915\n",
      "Epoch 50/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 716.5951 - val_loss: 5338.0083\n",
      "Epoch 51/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 723.0830 - val_loss: 5088.5386\n",
      "Epoch 52/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 687.9005 - val_loss: 4183.0581\n",
      "Epoch 53/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 711.1650 - val_loss: 7766.6572\n",
      "Epoch 54/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 610.2219 - val_loss: 6134.3545\n",
      "Epoch 55/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 595.9979 - val_loss: 4207.7852\n",
      "Epoch 56/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 676.3263 - val_loss: 3763.3865\n",
      "Epoch 57/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 701.7416 - val_loss: 4756.3105\n",
      "Epoch 58/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 622.2463 - val_loss: 3565.3149\n",
      "Epoch 59/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 548.4780 - val_loss: 4553.0303\n",
      "Epoch 60/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 586.0643 - val_loss: 3674.3970\n",
      "Epoch 61/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 542.5068 - val_loss: 3175.7385\n",
      "Epoch 62/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 576.0515 - val_loss: 2653.1140\n",
      "Epoch 63/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 619.6268 - val_loss: 2704.0317\n",
      "Epoch 64/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 568.6976 - val_loss: 2834.4075\n",
      "Epoch 65/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 564.8343 - val_loss: 3150.3035\n",
      "Epoch 66/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 494.6592 - val_loss: 2859.9680\n",
      "Epoch 67/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 524.3157 - val_loss: 2983.2463\n",
      "Epoch 68/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 552.5529 - val_loss: 3421.2061\n",
      "Epoch 69/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 586.8134 - val_loss: 3194.4324\n",
      "Epoch 70/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 586.3066 - val_loss: 3038.4248\n",
      "Epoch 71/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 597.2342 - val_loss: 2765.9871\n",
      "Epoch 72/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 608.7560 - val_loss: 3455.9270\n",
      "Epoch 73/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 573.6823 - val_loss: 2835.1763\n",
      "Epoch 74/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 561.1654 - val_loss: 3101.9417\n",
      "Epoch 75/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 496.0489 - val_loss: 2843.2986\n",
      "Epoch 76/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 404.1103 - val_loss: 2595.3743\n",
      "Epoch 77/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 506.7509 - val_loss: 2830.7480\n",
      "Epoch 78/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 531.4661 - val_loss: 3066.4543\n",
      "Epoch 79/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 451.7939 - val_loss: 2613.3604\n",
      "Epoch 80/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 468.0003 - val_loss: 2926.9402\n",
      "Epoch 81/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 436.9673 - val_loss: 2646.7896\n",
      "Epoch 82/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 465.1690 - val_loss: 2938.6228\n",
      "Epoch 83/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 467.9735 - val_loss: 2809.5300\n",
      "Epoch 84/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 458.4641 - val_loss: 2622.8416\n",
      "Epoch 85/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 461.0392 - val_loss: 2634.5896\n",
      "Epoch 86/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 469.7042 - val_loss: 2636.3518\n",
      "Epoch 87/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 440.7418 - val_loss: 2607.9155\n",
      "Epoch 88/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 465.3152 - val_loss: 2536.7568\n",
      "Epoch 89/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 445.9709 - val_loss: 2638.7864\n",
      "Epoch 90/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 457.1775 - val_loss: 2547.5413\n",
      "Epoch 91/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 444.0994 - val_loss: 2627.6687\n",
      "Epoch 92/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 412.0474 - val_loss: 2507.6702\n",
      "Epoch 93/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 411.3954 - val_loss: 2590.9111\n",
      "Epoch 94/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 374.5514 - val_loss: 2521.9875\n",
      "Epoch 95/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 408.6045 - val_loss: 2879.0137\n",
      "Epoch 96/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 453.7231 - val_loss: 2494.0710\n",
      "Epoch 97/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 399.4065 - val_loss: 2560.2583\n",
      "Epoch 98/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 388.4897 - val_loss: 2507.3035\n",
      "Epoch 99/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 443.3310 - val_loss: 2532.9531\n",
      "Epoch 100/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 371.1073 - val_loss: 2537.3665\n",
      "Epoch 101/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 369.2721 - val_loss: 2574.7456\n",
      "Epoch 102/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 349.1902 - val_loss: 2552.8533\n",
      "Epoch 103/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 383.5600 - val_loss: 2456.6655\n",
      "Epoch 104/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 366.0424 - val_loss: 2494.9924\n",
      "Epoch 105/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 347.0068 - val_loss: 2476.8726\n",
      "Epoch 106/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 365.0013 - val_loss: 2532.3850\n",
      "Epoch 107/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 340.8159 - val_loss: 2535.7107\n",
      "Epoch 108/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 414.8444 - val_loss: 2459.2747\n",
      "Epoch 109/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 369.9987 - val_loss: 2446.7070\n",
      "Epoch 110/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 387.2108 - val_loss: 2510.7048\n",
      "Epoch 111/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 405.4100 - val_loss: 2454.1309\n",
      "Epoch 112/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 352.2943 - val_loss: 2461.9895\n",
      "Epoch 113/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 359.5184 - val_loss: 2506.8635\n",
      "Epoch 114/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 302.3782 - val_loss: 2520.1440\n",
      "Epoch 115/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 354.3589 - val_loss: 2477.4629\n",
      "Epoch 116/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 425.8262 - val_loss: 2552.8245\n",
      "Epoch 117/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 382.3096 - val_loss: 2522.4353\n",
      "Epoch 118/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 385.8506 - val_loss: 2511.9534\n",
      "Epoch 119/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 370.1821 - val_loss: 2490.5383\n",
      "Epoch 120/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 320.4636 - val_loss: 2508.9666\n",
      "Epoch 121/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 354.4164 - val_loss: 2500.8740\n",
      "Epoch 122/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 364.3577 - val_loss: 2495.9250\n",
      "Epoch 123/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 353.9228 - val_loss: 2483.0010\n",
      "Epoch 124/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 351.4798 - val_loss: 2477.8354\n",
      "Epoch 125/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 328.5347 - val_loss: 2470.1892\n",
      "Epoch 126/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 358.0499 - val_loss: 2484.7734\n",
      "Epoch 127/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 390.3672 - val_loss: 2491.2136\n",
      "Epoch 128/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 349.3117 - val_loss: 2491.8425\n",
      "Epoch 129/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 406.8734 - val_loss: 2494.6067\n",
      "Epoch 130/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 356.5708 - val_loss: 2512.1379\n",
      "Epoch 131/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 374.7989 - val_loss: 2511.8540\n",
      "Epoch 132/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 346.2653 - val_loss: 2507.7039\n",
      "Epoch 133/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 331.8084 - val_loss: 2503.3633\n",
      "Epoch 134/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 369.8113 - val_loss: 2500.9937\n",
      "Epoch 135/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 374.1029 - val_loss: 2498.6021\n",
      "Epoch 136/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 357.0328 - val_loss: 2499.8879\n",
      "Epoch 137/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 345.6964 - val_loss: 2499.9717\n",
      "Epoch 138/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 328.3494 - val_loss: 2499.0496\n",
      "Epoch 139/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 358.5345 - val_loss: 2532.0605\n",
      "Epoch 140/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 468.3254 - val_loss: 5048.3711\n",
      "Epoch 141/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 446.0117 - val_loss: 3372.1604\n",
      "Epoch 142/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 552.1142 - val_loss: 2777.3198\n",
      "Epoch 143/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 652.4791 - val_loss: 2998.5408\n",
      "Epoch 144/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 579.6495 - val_loss: 2913.2485\n",
      "Epoch 145/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 980.4246 - val_loss: 4717.4854\n",
      "Epoch 146/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 872.7991 - val_loss: 2784.4128\n",
      "Epoch 147/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 679.6514 - val_loss: 3095.8262\n",
      "Epoch 148/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 661.1435 - val_loss: 3126.0527\n",
      "Epoch 149/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 651.6813 - val_loss: 5157.4326\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step\n",
      "\n",
      " Training Modell mit Seed 1\n",
      "Epoch 1/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - loss: 64283.6719 - val_loss: 114279.0859\n",
      "Epoch 2/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 13042.2197 - val_loss: 30276.0684\n",
      "Epoch 3/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4325.0112 - val_loss: 7098.5259\n",
      "Epoch 4/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 3045.2463 - val_loss: 5069.8911\n",
      "Epoch 5/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2652.2490 - val_loss: 10508.8379\n",
      "Epoch 6/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 2263.3113 - val_loss: 15362.6113\n",
      "Epoch 7/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 2232.5190 - val_loss: 11841.9199\n",
      "Epoch 8/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1911.2344 - val_loss: 10930.5840\n",
      "Epoch 9/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2041.3252 - val_loss: 11523.2354\n",
      "Epoch 10/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 2008.3979 - val_loss: 11397.6191\n",
      "Epoch 11/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1479.0112 - val_loss: 14217.0605\n",
      "Epoch 12/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1341.2297 - val_loss: 9751.4238\n",
      "Epoch 13/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1570.3328 - val_loss: 13632.5312\n",
      "Epoch 14/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1323.7305 - val_loss: 13014.2832\n",
      "Epoch 15/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1152.5057 - val_loss: 11488.5010\n",
      "Epoch 16/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1125.9218 - val_loss: 13918.6719\n",
      "Epoch 17/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1060.1924 - val_loss: 10259.9424\n",
      "Epoch 18/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1228.1089 - val_loss: 12567.9482\n",
      "Epoch 19/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1079.6699 - val_loss: 10919.3730\n",
      "Epoch 20/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1215.9594 - val_loss: 13506.6289\n",
      "Epoch 21/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1341.7904 - val_loss: 8774.1992\n",
      "Epoch 22/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 980.5811 - val_loss: 11690.0342\n",
      "Epoch 23/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1146.5345 - val_loss: 13121.4854\n",
      "Epoch 24/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1051.9308 - val_loss: 11633.0117\n",
      "Epoch 25/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1035.2279 - val_loss: 12946.5889\n",
      "Epoch 26/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1042.7041 - val_loss: 10966.5664\n",
      "Epoch 27/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 918.7218 - val_loss: 9528.5967\n",
      "Epoch 28/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1042.3008 - val_loss: 11017.8662\n",
      "Epoch 29/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1055.3032 - val_loss: 13559.6895\n",
      "Epoch 30/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1192.2550 - val_loss: 13832.0137\n",
      "Epoch 31/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 926.0442 - val_loss: 14351.3398\n",
      "Epoch 32/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 857.1478 - val_loss: 12963.7480\n",
      "Epoch 33/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 849.9421 - val_loss: 15063.4609\n",
      "Epoch 34/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 874.8221 - val_loss: 14560.4785\n",
      "Epoch 35/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 819.6456 - val_loss: 11637.6113\n",
      "Epoch 36/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 839.4465 - val_loss: 13519.2490\n",
      "Epoch 37/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 954.5433 - val_loss: 8897.4082\n",
      "Epoch 38/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1000.0327 - val_loss: 7665.1704\n",
      "Epoch 39/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 962.2271 - val_loss: 9105.4814\n",
      "Epoch 40/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 843.0530 - val_loss: 11959.8750\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
      "\n",
      " Training Modell mit Seed 2\n",
      "Epoch 1/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - loss: 71058.4688 - val_loss: 82245.5156\n",
      "Epoch 2/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 22061.0938 - val_loss: 30657.0879\n",
      "Epoch 3/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 5238.5957 - val_loss: 5553.7676\n",
      "Epoch 4/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 3278.9946 - val_loss: 6229.1504\n",
      "Epoch 5/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 2612.1345 - val_loss: 10032.5459\n",
      "Epoch 6/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 2447.2754 - val_loss: 9822.2158\n",
      "Epoch 7/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2032.4125 - val_loss: 9528.0801\n",
      "Epoch 8/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1701.7555 - val_loss: 9167.0684\n",
      "Epoch 9/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 2150.7839 - val_loss: 11204.0791\n",
      "Epoch 10/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1861.8369 - val_loss: 13799.9697\n",
      "Epoch 11/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1712.3142 - val_loss: 11624.4248\n",
      "Epoch 12/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1463.3381 - val_loss: 10633.7393\n",
      "Epoch 13/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1548.8567 - val_loss: 11644.8525\n",
      "Epoch 14/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1349.9796 - val_loss: 13170.4863\n",
      "Epoch 15/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1150.8887 - val_loss: 12010.7393\n",
      "Epoch 16/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1074.5807 - val_loss: 9849.7461\n",
      "Epoch 17/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1069.9110 - val_loss: 10935.9922\n",
      "Epoch 18/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1274.2003 - val_loss: 10332.1973\n",
      "Epoch 19/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1050.0342 - val_loss: 11022.6279\n",
      "Epoch 20/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1163.0206 - val_loss: 10137.5322\n",
      "Epoch 21/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1322.4342 - val_loss: 7481.5601\n",
      "Epoch 22/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1129.5907 - val_loss: 9086.3389\n",
      "Epoch 23/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1190.5415 - val_loss: 8723.0430\n",
      "Epoch 24/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1176.2705 - val_loss: 7206.1943\n",
      "Epoch 25/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1162.3103 - val_loss: 6955.7427\n",
      "Epoch 26/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1040.1533 - val_loss: 12146.2432\n",
      "Epoch 27/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1007.2419 - val_loss: 10244.4443\n",
      "Epoch 28/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 938.6188 - val_loss: 8076.6631\n",
      "Epoch 29/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 905.3246 - val_loss: 10518.4277\n",
      "Epoch 30/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 968.1617 - val_loss: 9517.7734\n",
      "Epoch 31/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 953.6266 - val_loss: 10658.0840\n",
      "Epoch 32/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 949.0140 - val_loss: 8726.9883\n",
      "Epoch 33/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 873.0955 - val_loss: 9394.6016\n",
      "Epoch 34/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 889.0411 - val_loss: 8629.4014\n",
      "Epoch 35/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 914.1974 - val_loss: 10386.1289\n",
      "Epoch 36/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 735.5114 - val_loss: 7679.6309\n",
      "Epoch 37/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 880.4304 - val_loss: 10061.8066\n",
      "Epoch 38/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 769.9286 - val_loss: 7451.7300\n",
      "Epoch 39/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 829.8486 - val_loss: 8512.5703\n",
      "Epoch 40/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 785.6447 - val_loss: 7851.9688\n",
      "WARNING:tensorflow:5 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017173680B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\n",
      " Training Modell mit Seed 3\n",
      "Epoch 1/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - loss: 68284.5312 - val_loss: 105568.7344\n",
      "Epoch 2/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 18775.4551 - val_loss: 10715.3096\n",
      "Epoch 3/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 4671.2183 - val_loss: 6667.3584\n",
      "Epoch 4/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2933.2151 - val_loss: 8587.1934\n",
      "Epoch 5/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2642.7839 - val_loss: 15585.5664\n",
      "Epoch 6/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2139.1228 - val_loss: 14309.4453\n",
      "Epoch 7/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2117.1523 - val_loss: 15344.8818\n",
      "Epoch 8/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1887.9861 - val_loss: 14192.4736\n",
      "Epoch 9/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1780.9507 - val_loss: 10600.0586\n",
      "Epoch 10/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1616.2694 - val_loss: 12876.4551\n",
      "Epoch 11/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1520.3651 - val_loss: 11979.3760\n",
      "Epoch 12/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1399.7587 - val_loss: 10642.3447\n",
      "Epoch 13/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1417.2310 - val_loss: 15422.2666\n",
      "Epoch 14/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1485.4939 - val_loss: 8486.6543\n",
      "Epoch 15/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1579.0110 - val_loss: 8700.9941\n",
      "Epoch 16/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1614.7767 - val_loss: 11236.7080\n",
      "Epoch 17/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1316.8629 - val_loss: 13183.0723\n",
      "Epoch 18/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1505.7692 - val_loss: 8785.9092\n",
      "Epoch 19/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1228.9847 - val_loss: 11210.3809\n",
      "Epoch 20/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1158.2156 - val_loss: 8652.4775\n",
      "Epoch 21/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1128.4912 - val_loss: 9586.0293\n",
      "Epoch 22/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1008.5209 - val_loss: 10707.8721\n",
      "Epoch 23/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1005.1852 - val_loss: 8803.2500\n",
      "Epoch 24/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1100.8038 - val_loss: 9424.3975\n",
      "Epoch 25/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1002.7575 - val_loss: 10826.1494\n",
      "Epoch 26/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 955.1838 - val_loss: 9358.3350\n",
      "Epoch 27/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 957.0283 - val_loss: 11982.3955\n",
      "Epoch 28/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 899.6888 - val_loss: 10245.4277\n",
      "Epoch 29/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1121.1168 - val_loss: 7483.5508\n",
      "Epoch 30/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1034.7194 - val_loss: 10939.2354\n",
      "Epoch 31/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 921.5512 - val_loss: 8934.4688\n",
      "Epoch 32/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 993.7426 - val_loss: 8608.7158\n",
      "Epoch 33/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 915.3511 - val_loss: 10063.1396\n",
      "Epoch 34/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 941.5259 - val_loss: 9448.6914\n",
      "Epoch 35/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 947.9055 - val_loss: 9572.1113\n",
      "Epoch 36/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 864.2489 - val_loss: 6029.8228\n",
      "Epoch 37/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1041.9507 - val_loss: 9525.7744\n",
      "Epoch 38/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 906.1300 - val_loss: 7233.5352\n",
      "Epoch 39/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 893.7706 - val_loss: 8789.9160\n",
      "Epoch 40/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 755.2223 - val_loss: 10313.7773\n",
      "WARNING:tensorflow:6 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017173D5B420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\n",
      " Training Modell mit Seed 4\n",
      "Epoch 1/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 70793.0469 - val_loss: 22532.9902\n",
      "Epoch 2/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 19589.8945 - val_loss: 11347.5127\n",
      "Epoch 3/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 4032.5002 - val_loss: 8661.0283\n",
      "Epoch 4/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 3118.7642 - val_loss: 20065.8262\n",
      "Epoch 5/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2603.7832 - val_loss: 18752.6777\n",
      "Epoch 6/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2082.2830 - val_loss: 14152.8184\n",
      "Epoch 7/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 2014.0532 - val_loss: 8812.2178\n",
      "Epoch 8/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 2006.3622 - val_loss: 14252.7031\n",
      "Epoch 9/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1829.2017 - val_loss: 13727.3848\n",
      "Epoch 10/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1795.8433 - val_loss: 10944.6992\n",
      "Epoch 11/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1799.2798 - val_loss: 9473.3525\n",
      "Epoch 12/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1351.3094 - val_loss: 9030.6826\n",
      "Epoch 13/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1230.0402 - val_loss: 9608.0791\n",
      "Epoch 14/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1432.6586 - val_loss: 9403.3018\n",
      "Epoch 15/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1247.6478 - val_loss: 10026.4258\n",
      "Epoch 16/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1370.3076 - val_loss: 7912.8193\n",
      "Epoch 17/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1251.2704 - val_loss: 7874.4941\n",
      "Epoch 18/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1058.2123 - val_loss: 6804.7583\n",
      "Epoch 19/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1136.7125 - val_loss: 8306.9053\n",
      "Epoch 20/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1217.3406 - val_loss: 10862.6143\n",
      "Epoch 21/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1382.1945 - val_loss: 9015.6846\n",
      "Epoch 22/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1441.3931 - val_loss: 10464.2881\n",
      "Epoch 23/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1108.0103 - val_loss: 11473.5811\n",
      "Epoch 24/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1072.4917 - val_loss: 11707.8730\n",
      "Epoch 25/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1272.1892 - val_loss: 11027.0654\n",
      "Epoch 26/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1185.1556 - val_loss: 12154.0273\n",
      "Epoch 27/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1000.0108 - val_loss: 8121.9888\n",
      "Epoch 28/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1131.3052 - val_loss: 6164.8652\n",
      "Epoch 29/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1073.3616 - val_loss: 11029.7891\n",
      "Epoch 30/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1119.1641 - val_loss: 8885.0850\n",
      "Epoch 31/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1053.4622 - val_loss: 9427.2168\n",
      "Epoch 32/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1100.0236 - val_loss: 10726.6367\n",
      "Epoch 33/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 997.7936 - val_loss: 8267.0342\n",
      "Epoch 34/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 930.4476 - val_loss: 8271.7822\n",
      "Epoch 35/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 850.0925 - val_loss: 9553.5762\n",
      "Epoch 36/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 902.3043 - val_loss: 8059.5649\n",
      "Epoch 37/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 985.2111 - val_loss: 9385.5547\n",
      "Epoch 38/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 770.2083 - val_loss: 9098.2754\n",
      "Epoch 39/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 798.7771 - val_loss: 10862.1777\n",
      "Epoch 40/400\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 966.8549 - val_loss: 8453.8818\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n"
     ]
    }
   ],
   "source": [
    "#optimization of the code above\n",
    "#add dropout, cosine LR Decay, layer setup, early stopping, 5 times ensemble\n",
    "\n",
    "#massive overfitting !!!!!\n",
    "batch_size = 128\n",
    "\n",
    "# Validation Split (15%)\n",
    "val_split = int(len(X_train) * 0.15)\n",
    "X_val = X_train[:val_split]\n",
    "y_val = y_train[:val_split]\n",
    "X_tr = X_train[val_split:]\n",
    "y_tr = y_train[val_split:]\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_tr, y_tr.astype(\"float32\")))\n",
    "ds_train = ds_train.shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((X_val, y_val.astype(\"float32\")))\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "#Cosine LR Decay\n",
    "def get_optimizer():\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate=1e-3,\n",
    "        first_decay_steps=2500,\n",
    "        t_mul=1.5,\n",
    "        m_mul=0.9,\n",
    "        alpha=1e-4,\n",
    "    )\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "\n",
    "#Modell-Builder\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_features,)),\n",
    "\n",
    "        tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.15),\n",
    "\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=get_optimizer(),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#training\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=40,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Ensemble (5 Models)\n",
    "all_preds = []\n",
    "\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    print(f\"Training Modell with Seed {seed}\")\n",
    "\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=400,\n",
    "        verbose=1,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_test, batch_size=1024).flatten()\n",
    "    all_preds.append(preds)\n",
    "\n",
    "# mean of Predictions\n",
    "final_pred = np.mean(all_preds, axis=0)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": final_pred\n",
    "})\n",
    "\n",
    "#sub.to_csv(\"Submissions/predictions_NN_ensemble.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f81ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 51810.1133 - val_loss: 10175.5322\n",
      "Epoch 2/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13745.8955 - val_loss: 9917.3125\n",
      "Epoch 3/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10854.4033 - val_loss: 7695.5195\n",
      "Epoch 4/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9744.0000 - val_loss: 7091.2080\n",
      "Epoch 5/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8893.9043 - val_loss: 6522.5488\n",
      "Epoch 6/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8198.1670 - val_loss: 5828.9263\n",
      "Epoch 7/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8035.3452 - val_loss: 5510.0679\n",
      "Epoch 8/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7419.2622 - val_loss: 5596.1387\n",
      "Epoch 9/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6593.2725 - val_loss: 4522.7803\n",
      "Epoch 10/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6137.7695 - val_loss: 4688.0723\n",
      "Epoch 11/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5442.3335 - val_loss: 4101.7539\n",
      "Epoch 12/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4928.6523 - val_loss: 4319.6973\n",
      "Epoch 13/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4812.2622 - val_loss: 3779.3372\n",
      "Epoch 14/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4600.8271 - val_loss: 3636.8616\n",
      "Epoch 15/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4262.4531 - val_loss: 5309.7632\n",
      "Epoch 16/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4578.5967 - val_loss: 4165.9419\n",
      "Epoch 17/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3897.3779 - val_loss: 3327.7522\n",
      "Epoch 18/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3804.1665 - val_loss: 3604.5242\n",
      "Epoch 19/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3736.8086 - val_loss: 4085.7637\n",
      "Epoch 20/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3555.6201 - val_loss: 3544.1147\n",
      "Epoch 21/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3496.3879 - val_loss: 3755.3533\n",
      "Epoch 22/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3358.9150 - val_loss: 3245.8140\n",
      "Epoch 23/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3648.0781 - val_loss: 4163.9106\n",
      "Epoch 24/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3268.7749 - val_loss: 3983.5044\n",
      "Epoch 25/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3202.1440 - val_loss: 3385.4138\n",
      "Epoch 26/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3233.0618 - val_loss: 3097.0015\n",
      "Epoch 27/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3098.3567 - val_loss: 3276.9111\n",
      "Epoch 28/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3045.9888 - val_loss: 3088.9561\n",
      "Epoch 29/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2975.6895 - val_loss: 3282.4097\n",
      "Epoch 30/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2764.0557 - val_loss: 3271.1174\n",
      "Epoch 31/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2822.1367 - val_loss: 3558.1672\n",
      "Epoch 32/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2735.3420 - val_loss: 3721.8120\n",
      "Epoch 33/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2758.0962 - val_loss: 3767.5361\n",
      "Epoch 34/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2648.8220 - val_loss: 3540.2512\n",
      "Epoch 35/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2697.0471 - val_loss: 3513.3696\n",
      "Epoch 36/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2626.3225 - val_loss: 3113.2053\n",
      "Epoch 37/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2552.1284 - val_loss: 3627.4329\n",
      "Epoch 38/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2569.3269 - val_loss: 3912.4568\n",
      "Epoch 39/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2621.9302 - val_loss: 4692.2305\n",
      "Epoch 40/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2676.3469 - val_loss: 3608.8110\n",
      "Epoch 41/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2156.3750 - val_loss: 3138.6604\n",
      "Epoch 42/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2412.5610 - val_loss: 3166.0166\n",
      "Epoch 43/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2407.3406 - val_loss: 3501.0684\n",
      "Epoch 44/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2297.9856 - val_loss: 3332.2334\n",
      "Epoch 45/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2189.3162 - val_loss: 3228.5444\n",
      "Epoch 46/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2176.8325 - val_loss: 3195.9329\n",
      "Epoch 47/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2198.5010 - val_loss: 3459.8140\n",
      "Epoch 48/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2213.3782 - val_loss: 3234.6018\n",
      "Epoch 49/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2157.4487 - val_loss: 3597.2166\n",
      "Epoch 50/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2174.3940 - val_loss: 3244.0906\n",
      "Epoch 51/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2069.7793 - val_loss: 3761.9595\n",
      "Epoch 52/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2232.9263 - val_loss: 3411.6462\n",
      "Epoch 53/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1987.4207 - val_loss: 3426.0073\n",
      "Epoch 54/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2012.9296 - val_loss: 3194.3555\n",
      "Epoch 55/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2037.7433 - val_loss: 3241.3604\n",
      "Epoch 56/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2121.1584 - val_loss: 3285.7717\n",
      "Epoch 57/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2186.8137 - val_loss: 3445.9370\n",
      "Epoch 58/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1959.6699 - val_loss: 3450.0225\n",
      "Epoch 59/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2018.8365 - val_loss: 3347.7629\n",
      "Epoch 60/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2034.5535 - val_loss: 3490.1804\n",
      "Epoch 61/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2061.6453 - val_loss: 3206.0720\n",
      "Epoch 62/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1859.1372 - val_loss: 3796.9370\n",
      "Epoch 63/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1960.2935 - val_loss: 3267.1553\n",
      "Epoch 64/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1806.0343 - val_loss: 3237.4927\n",
      "Epoch 65/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2017.8080 - val_loss: 3355.9429\n",
      "Epoch 66/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1897.5594 - val_loss: 3216.5220\n",
      "Epoch 67/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1985.3734 - val_loss: 3997.0840\n",
      "Epoch 68/300\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2068.0173 - val_loss: 3260.5989\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>357.531769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1146</td>\n",
       "      <td>346.450684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>206.580292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2279</td>\n",
       "      <td>177.256149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342</td>\n",
       "      <td>236.501114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2663</td>\n",
       "      <td>313.843353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>624</td>\n",
       "      <td>301.870972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2655</td>\n",
       "      <td>229.336014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2089</td>\n",
       "      <td>262.117554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1065</td>\n",
       "      <td>243.461838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>666 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          Tm\n",
       "0    1022  357.531769\n",
       "1    1146  346.450684\n",
       "2      79  206.580292\n",
       "3    2279  177.256149\n",
       "4    1342  236.501114\n",
       "..    ...         ...\n",
       "661  2663  313.843353\n",
       "662   624  301.870972\n",
       "663  2655  229.336014\n",
       "664  2089  262.117554\n",
       "665  1065  243.461838\n",
       "\n",
       "[666 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not as good as the second approach\n",
    "batch_size = 128\n",
    "val_split = int(len(X_train) * 0.15)\n",
    "\n",
    "X_val = X_train[:val_split]\n",
    "y_val = y_train[:val_split]\n",
    "X_tr = X_train[val_split:]\n",
    "y_tr = y_train[val_split:]\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_tr, y_tr.astype(\"float32\")))\n",
    "ds_train = ds_train.shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((X_val, y_val.astype(\"float32\")))\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Anti-Overfitting Model\n",
    "def build_model():\n",
    "    reg = tf.keras.regularizers.l2(1e-5)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input((X_train.shape[1],)),\n",
    "\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=reg),\n",
    "        tf.keras.layers.Dropout(0.30),\n",
    "\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=reg),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=reg),\n",
    "        tf.keras.layers.Dropout(0.20),\n",
    "\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=reg),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Training\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=40,\n",
    "    restore_best_weights=True,\n",
    "    monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=300,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "pred = model.predict(X_test).flatten()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": pred\n",
    "})#.to_csv(\"Submissions/predictions_NN_regularized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f1e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial:\n",
      "{'n_hidden_1': 315, 'n_hidden_2': 424, 'n_hidden_3': 205, 'learning_rate': 0.0006846199681208632, 'dropout': 0.14641949270018959, 'batch_size': 64, 'activation': 'gelu'}\n",
      "Epoch 1/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 24961.8086\n",
      "Epoch 2/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 7865.4448\n",
      "Epoch 3/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5741.5264\n",
      "Epoch 4/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4138.1787\n",
      "Epoch 5/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3454.4573\n",
      "Epoch 6/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3025.1982\n",
      "Epoch 7/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2593.9482\n",
      "Epoch 8/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2410.1074\n",
      "Epoch 9/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2145.7219\n",
      "Epoch 10/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2071.3965\n",
      "Epoch 11/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1961.5610\n",
      "Epoch 12/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1785.6409\n",
      "Epoch 13/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1671.2166\n",
      "Epoch 14/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1551.0464\n",
      "Epoch 15/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1366.6785\n",
      "Epoch 16/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1305.8005\n",
      "Epoch 17/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1209.5175\n",
      "Epoch 18/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1092.7344\n",
      "Epoch 19/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1102.7928\n",
      "Epoch 20/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1051.4557\n",
      "Epoch 21/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1039.4429\n",
      "Epoch 22/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 990.6461\n",
      "Epoch 23/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 948.9921\n",
      "Epoch 24/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 848.6518\n",
      "Epoch 25/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 899.5121\n",
      "Epoch 26/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 780.0475\n",
      "Epoch 27/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 822.4612\n",
      "Epoch 28/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 843.0237\n",
      "Epoch 29/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 665.4827\n",
      "Epoch 30/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 644.1156\n",
      "Epoch 31/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 615.5708\n",
      "Epoch 32/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 677.7968\n",
      "Epoch 33/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 543.3329\n",
      "Epoch 34/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 604.1892\n",
      "Epoch 35/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 627.4028\n",
      "Epoch 36/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 567.1125\n",
      "Epoch 37/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 494.0944\n",
      "Epoch 38/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 540.7326\n",
      "Epoch 39/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 474.5465\n",
      "Epoch 40/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 505.5453\n",
      "Epoch 41/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 450.4754\n",
      "Epoch 42/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 483.6344\n",
      "Epoch 43/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 539.6585\n",
      "Epoch 44/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 604.3226\n",
      "Epoch 45/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 469.2212\n",
      "Epoch 46/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 431.6803\n",
      "Epoch 47/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 472.3159\n",
      "Epoch 48/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 516.2851\n",
      "Epoch 49/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 450.9880\n",
      "Epoch 50/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 523.7825\n",
      "Epoch 51/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 428.1575\n",
      "Epoch 52/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 395.4464\n",
      "Epoch 53/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 395.6926\n",
      "Epoch 54/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 363.8426\n",
      "Epoch 55/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 343.9913\n",
      "Epoch 56/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 385.1555\n",
      "Epoch 57/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 409.4374\n",
      "Epoch 58/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 369.4413\n",
      "Epoch 59/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 345.2565\n",
      "Epoch 60/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 431.5175\n",
      "Epoch 61/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 410.2743\n",
      "Epoch 62/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 398.1681\n",
      "Epoch 63/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 396.4255\n",
      "Epoch 64/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 375.8194\n",
      "Epoch 65/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 394.4806\n",
      "Epoch 66/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 407.6777\n",
      "Epoch 67/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 417.2168\n",
      "Epoch 68/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 345.4339\n",
      "Epoch 69/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 343.6870\n",
      "Epoch 70/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 301.6211\n",
      "Epoch 71/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 317.7387\n",
      "Epoch 72/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 357.5313\n",
      "Epoch 73/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 360.6412\n",
      "Epoch 74/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 357.3792\n",
      "Epoch 75/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 345.8383\n",
      "Epoch 76/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 286.5030\n",
      "Epoch 77/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 351.2389\n",
      "Epoch 78/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 323.2423\n",
      "Epoch 79/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 320.3873\n",
      "Epoch 80/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 254.1764\n",
      "Epoch 81/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 313.0207\n",
      "Epoch 82/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 330.7450\n",
      "Epoch 83/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 424.2833\n",
      "Epoch 84/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 314.2420\n",
      "Epoch 85/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 294.5649\n",
      "Epoch 86/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 295.6967\n",
      "Epoch 87/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 299.1518\n",
      "Epoch 88/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 310.7575\n",
      "Epoch 89/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 344.5962\n",
      "Epoch 90/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 276.0110\n",
      "Epoch 91/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 286.8567\n",
      "Epoch 92/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 376.0085\n",
      "Epoch 93/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 294.9622\n",
      "Epoch 94/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 276.0105\n",
      "Epoch 95/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 341.3279\n",
      "Epoch 96/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 267.5601\n",
      "Epoch 97/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 270.0969\n",
      "Epoch 98/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 251.4117\n",
      "Epoch 99/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 297.3982\n",
      "Epoch 100/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 296.0311\n",
      "Epoch 101/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 298.8510\n",
      "Epoch 102/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 281.9697\n",
      "Epoch 103/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 272.9189\n",
      "Epoch 104/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 307.9149\n",
      "Epoch 105/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 256.2419\n",
      "Epoch 106/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 259.1923\n",
      "Epoch 107/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 269.9121\n",
      "Epoch 108/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 352.4815\n",
      "Epoch 109/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 298.1306\n",
      "Epoch 110/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 321.9894\n",
      "Epoch 111/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 307.1735\n",
      "Epoch 112/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 313.4249\n",
      "Epoch 113/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 258.0056\n",
      "Epoch 114/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 282.3260\n",
      "Epoch 115/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 252.0062\n",
      "Epoch 116/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 295.3209\n",
      "Epoch 117/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 286.2375\n",
      "Epoch 118/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 277.0828\n",
      "Epoch 119/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 266.4127\n",
      "Epoch 120/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 241.9956\n",
      "Epoch 121/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 253.8628\n",
      "Epoch 122/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 295.4476\n",
      "Epoch 123/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 286.0608\n",
      "Epoch 124/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 412.6422\n",
      "Epoch 125/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 255.7224\n",
      "Epoch 126/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 236.4167\n",
      "Epoch 127/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 310.0923\n",
      "Epoch 128/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 280.3953\n",
      "Epoch 129/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 266.2734\n",
      "Epoch 130/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 238.3995\n",
      "Epoch 131/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 256.9060\n",
      "Epoch 132/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 231.5993\n",
      "Epoch 133/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 262.0071\n",
      "Epoch 134/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 202.2685\n",
      "Epoch 135/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 246.6245\n",
      "Epoch 136/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 241.5196\n",
      "Epoch 137/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 241.3835\n",
      "Epoch 138/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 231.9803\n",
      "Epoch 139/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 266.3238\n",
      "Epoch 140/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 288.1390\n",
      "Epoch 141/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 321.7568\n",
      "Epoch 142/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 285.8400\n",
      "Epoch 143/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 252.7126\n",
      "Epoch 144/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 223.2901\n",
      "Epoch 145/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 200.6997\n",
      "Epoch 146/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 223.1765\n",
      "Epoch 147/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 205.2505\n",
      "Epoch 148/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 232.5829\n",
      "Epoch 149/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 208.3922\n",
      "Epoch 150/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 296.3924\n",
      "Epoch 151/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 294.5268\n",
      "Epoch 152/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 260.8403\n",
      "Epoch 153/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 240.5288\n",
      "Epoch 154/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 220.8141\n",
      "Epoch 155/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 229.4169\n",
      "Epoch 156/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.2349\n",
      "Epoch 157/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 285.6877\n",
      "Epoch 158/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 260.0466\n",
      "Epoch 159/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 204.4693\n",
      "Epoch 160/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 247.7052\n",
      "Epoch 161/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 252.7498\n",
      "Epoch 162/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 287.3616\n",
      "Epoch 163/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 247.0516\n",
      "Epoch 164/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 189.0310\n",
      "Epoch 165/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 231.8180\n",
      "Epoch 166/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 238.7348\n",
      "Epoch 167/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 233.8364\n",
      "Epoch 168/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 217.3843\n",
      "Epoch 169/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 215.5301\n",
      "Epoch 170/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 227.0440\n",
      "Epoch 171/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 258.9636\n",
      "Epoch 172/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 193.6157\n",
      "Epoch 173/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 215.5668\n",
      "Epoch 174/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 173.3710\n",
      "Epoch 175/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 248.1228\n",
      "Epoch 176/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 206.9220\n",
      "Epoch 177/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 206.1196\n",
      "Epoch 178/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 260.4980\n",
      "Epoch 179/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 218.6388\n",
      "Epoch 180/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 212.0945\n",
      "Epoch 181/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 230.8749\n",
      "Epoch 182/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 292.0193\n",
      "Epoch 183/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 270.3693\n",
      "Epoch 184/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 218.2953\n",
      "Epoch 185/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 234.1262\n",
      "Epoch 186/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 246.2591\n",
      "Epoch 187/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 183.5382\n",
      "Epoch 188/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 233.1505\n",
      "Epoch 189/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 205.0582\n",
      "Epoch 190/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 247.5056\n",
      "Epoch 191/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 191.9763\n",
      "Epoch 192/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 213.8227\n",
      "Epoch 193/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 185.0334\n",
      "Epoch 194/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 234.3493\n",
      "Epoch 195/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 224.2663\n",
      "Epoch 196/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 197.9505\n",
      "Epoch 197/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 198.9116\n",
      "Epoch 198/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 215.4161\n",
      "Epoch 199/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 196.8932\n",
      "Epoch 200/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 236.2836\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022</td>\n",
       "      <td>340.564850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1146</td>\n",
       "      <td>265.474060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>227.070847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2279</td>\n",
       "      <td>209.710571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1342</td>\n",
       "      <td>239.315216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>2663</td>\n",
       "      <td>277.880676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>624</td>\n",
       "      <td>283.993744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2655</td>\n",
       "      <td>232.423004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2089</td>\n",
       "      <td>228.327103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>1065</td>\n",
       "      <td>257.727448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>666 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          Tm\n",
       "0    1022  340.564850\n",
       "1    1146  265.474060\n",
       "2      79  227.070847\n",
       "3    2279  209.710571\n",
       "4    1342  239.315216\n",
       "..    ...         ...\n",
       "661  2663  277.880676\n",
       "662   624  283.993744\n",
       "663  2655  232.423004\n",
       "664  2089  228.327103\n",
       "665  1065  257.727448\n",
       "\n",
       "[666 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage of optuna\n",
    "#is worse than the second try\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Data\n",
    "\n",
    "X_train_np = X_train.astype(\"float32\")\n",
    "y_train_np = y_train.astype(\"float32\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_np, y_train_np, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# TF Datasets Funktion\n",
    "def make_dataset(X, y, batch_size):\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        .shuffle(len(X))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# Optuna \n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameter search\n",
    "    n1 = trial.suggest_int(\"n_hidden_1\", 128, 1024)\n",
    "    n2 = trial.suggest_int(\"n_hidden_2\", 64, 512)\n",
    "    n3 = trial.suggest_int(\"n_hidden_3\", 0, 256)   # 0 = kein Layer 3\n",
    "\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"gelu\"])\n",
    "\n",
    "    # Dataset\n",
    "    ds_train = make_dataset(X_tr, y_tr, batch_size)\n",
    "    ds_val = make_dataset(X_val, y_val, batch_size)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(X_tr.shape[1],)))\n",
    "    model.add(tf.keras.layers.Dense(n1, activation=activation))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    model.add(tf.keras.layers.Dense(n2, activation=activation))\n",
    "\n",
    "    if n3 > 0:\n",
    "        model.add(tf.keras.layers.Dense(n3, activation=activation))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    # Training \n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=40,        # kurz halten, wird oft ausgefhrt\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # goal: Minimal Val-MSE\n",
    "    val_loss = min(history.history[\"val_loss\"])\n",
    "    return val_loss\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40)   # 4080 empfohlen\n",
    "\n",
    "print(\"\\nBest Trial:\")\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# FINAL MODEL with best parameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "n1 = best_params[\"n_hidden_1\"]\n",
    "n2 = best_params[\"n_hidden_2\"]\n",
    "n3 = best_params[\"n_hidden_3\"]\n",
    "lr = best_params[\"learning_rate\"]\n",
    "dropout = best_params[\"dropout\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "activation = best_params[\"activation\"]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(X_train_np.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(n1, activation=activation))\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "model.add(tf.keras.layers.Dense(n2, activation=activation))\n",
    "if n3 > 0:\n",
    "    model.add(tf.keras.layers.Dense(n3, activation=activation))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "ds_full = make_dataset(X_train_np, y_train_np, batch_size)\n",
    "\n",
    "# full training\n",
    "history = model.fit(\n",
    "    ds_full,\n",
    "    epochs=200,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Prediction\n",
    "pred_test = model.predict(X_test.astype(\"float32\")).flatten()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": pred_test\n",
    "})#.to_csv(\"Submissions/predictions_NN_optuna.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08467232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                       SMILES      Tm  Group 1  Group 2  Group 3  \\\n",
      "0  2175        FC1=C(F)C(F)(F)C1(F)F  213.15        0        0        0   \n",
      "1  1222  c1ccc2c(c1)ccc3Nc4ccccc4c23  407.15        0        0        0   \n",
      "2  2994          CCN1C(C)=Nc2ccccc12  324.15        2        1        0   \n",
      "3  1704                   CC#CC(=O)O  351.15        1        0        0   \n",
      "4  2526                    CCCCC(S)C  126.15        2        3        0   \n",
      "\n",
      "   Group 4  Group 5  Group 6  Group 7  ...  Group 415  Group 416  Group 417  \\\n",
      "0        0        0        0        0  ...          0          0          0   \n",
      "1        0        0        0        0  ...          0          0          0   \n",
      "2        0        0        0        0  ...          0          0          0   \n",
      "3        0        0        0        0  ...          0          0          0   \n",
      "4        0        0        0        0  ...          0          0          0   \n",
      "\n",
      "   Group 418  Group 419  Group 420  Group 421  Group 422  Group 423  Group 424  \n",
      "0          0          0          0          0          0          0          0  \n",
      "1          0          0          0          0          0          0          0  \n",
      "2          0          0          0          0          0          0          0  \n",
      "3          0          0          0          0          0          0          0  \n",
      "4          0          0          0          0          0          0          0  \n",
      "\n",
      "[5 rows x 427 columns]\n",
      "Epoch 1/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 80296.7969\n",
      "Epoch 2/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 68274.5703\n",
      "Epoch 3/300\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 55918.7422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwind\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 47625.6641\n",
      "Epoch 4/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24744.9746\n",
      "Epoch 5/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9116.2197 \n",
      "Epoch 6/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3465.7000\n",
      "Epoch 7/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2354.4543\n",
      "Epoch 8/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2660.3381\n",
      "Epoch 9/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2316.0029\n",
      "Epoch 10/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2808.6179\n",
      "Epoch 11/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2360.6787\n",
      "Epoch 12/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1658.4675\n",
      "Epoch 13/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2046.4220\n",
      "Epoch 14/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1976.5376\n",
      "Epoch 15/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1622.5945\n",
      "Epoch 16/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1825.8505\n",
      "Epoch 17/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1870.8082\n",
      "Epoch 18/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2144.5784\n",
      "Epoch 19/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2099.9126\n",
      "Epoch 20/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1413.8153\n",
      "Epoch 21/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1559.3759\n",
      "Epoch 22/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1883.6134\n",
      "Epoch 23/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1393.2628\n",
      "Epoch 24/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1740.1228\n",
      "Epoch 25/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1863.0918\n",
      "Epoch 26/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1546.2709\n",
      "Epoch 27/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1578.9523\n",
      "Epoch 28/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1371.2751\n",
      "Epoch 29/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1533.5470\n",
      "Epoch 30/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1369.6521\n",
      "Epoch 31/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1619.7179\n",
      "Epoch 32/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1040.2334\n",
      "Epoch 33/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1357.2498\n",
      "Epoch 34/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1393.9192\n",
      "Epoch 35/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1188.4618\n",
      "Epoch 36/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1731.4631\n",
      "Epoch 37/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1358.6100\n",
      "Epoch 38/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1129.3589\n",
      "Epoch 39/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1518.5682\n",
      "Epoch 40/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1539.0997\n",
      "Epoch 41/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1182.4799\n",
      "Epoch 42/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1583.8096\n",
      "Epoch 43/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1420.4705\n",
      "Epoch 44/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1199.6021\n",
      "Epoch 45/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1720.5579\n",
      "Epoch 46/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1047.8402\n",
      "Epoch 47/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1121.1179\n",
      "Epoch 48/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1327.2195\n",
      "Epoch 49/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1250.3218\n",
      "Epoch 50/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1249.0706\n",
      "Epoch 51/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1278.5076\n",
      "Epoch 52/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1162.7400\n",
      "Epoch 53/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1159.6948\n",
      "Epoch 54/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1174.5446\n",
      "Epoch 55/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1143.4603\n",
      "Epoch 56/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1209.7968\n",
      "Epoch 57/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 986.1060 \n",
      "Epoch 58/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1515.6635\n",
      "Epoch 59/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1212.1080\n",
      "Epoch 60/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1512.5903\n",
      "Epoch 61/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1357.1899\n",
      "Epoch 62/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1636.2222\n",
      "Epoch 63/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1683.7455\n",
      "Epoch 64/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1202.0974\n",
      "Epoch 65/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1265.1559\n",
      "Epoch 66/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1149.9929\n",
      "Epoch 67/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1475.4169\n",
      "Epoch 68/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 925.5596\n",
      "Epoch 69/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1312.8406\n",
      "Epoch 70/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1356.5540\n",
      "Epoch 71/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1314.7805\n",
      "Epoch 72/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1350.5018\n",
      "Epoch 73/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1257.3667\n",
      "Epoch 74/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1278.7642\n",
      "Epoch 75/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1520.7264\n",
      "Epoch 76/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 888.3745\n",
      "Epoch 77/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1045.8406\n",
      "Epoch 78/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 880.1451 \n",
      "Epoch 79/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 928.4885\n",
      "Epoch 80/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1096.2046\n",
      "Epoch 81/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1096.9232\n",
      "Epoch 82/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1056.3456\n",
      "Epoch 83/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 914.1027\n",
      "Epoch 84/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1265.5729\n",
      "Epoch 85/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1205.7583\n",
      "Epoch 86/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 928.4414\n",
      "Epoch 87/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1024.3527\n",
      "Epoch 88/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 962.6562\n",
      "Epoch 89/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1177.2969\n",
      "Epoch 90/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 967.1165\n",
      "Epoch 91/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 811.7632\n",
      "Epoch 92/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1061.8984\n",
      "Epoch 93/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1493.0023\n",
      "Epoch 94/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 846.9364\n",
      "Epoch 95/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 906.2205\n",
      "Epoch 96/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1036.0330\n",
      "Epoch 97/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1169.8375\n",
      "Epoch 98/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1101.6425\n",
      "Epoch 99/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1008.1513\n",
      "Epoch 100/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 924.3533\n",
      "Epoch 101/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 954.3553\n",
      "Epoch 102/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1070.9728\n",
      "Epoch 103/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090.6969\n",
      "Epoch 104/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1002.7692\n",
      "Epoch 105/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 762.5427\n",
      "Epoch 106/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1027.4547\n",
      "Epoch 107/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 882.1624 \n",
      "Epoch 108/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 858.5176\n",
      "Epoch 109/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 851.3322\n",
      "Epoch 110/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 861.9526\n",
      "Epoch 111/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 680.7454\n",
      "Epoch 112/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 814.2629\n",
      "Epoch 113/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 871.5876\n",
      "Epoch 114/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 899.3509\n",
      "Epoch 115/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 818.6129\n",
      "Epoch 116/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 722.5105 \n",
      "Epoch 117/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 889.7346\n",
      "Epoch 118/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 899.6304\n",
      "Epoch 119/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 805.8434\n",
      "Epoch 120/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 983.7657\n",
      "Epoch 121/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 854.1445\n",
      "Epoch 122/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 793.3508\n",
      "Epoch 123/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 903.1871\n",
      "Epoch 124/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 906.2591\n",
      "Epoch 125/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 776.4328\n",
      "Epoch 126/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 675.1278\n",
      "Epoch 127/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 756.9343\n",
      "Epoch 128/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 747.8624\n",
      "Epoch 129/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 730.2073\n",
      "Epoch 130/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 845.2239\n",
      "Epoch 131/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 754.2675\n",
      "Epoch 132/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 902.0903 \n",
      "Epoch 133/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 686.5945\n",
      "Epoch 134/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 698.4588\n",
      "Epoch 135/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 694.6246\n",
      "Epoch 136/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 658.0764\n",
      "Epoch 137/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 823.5361\n",
      "Epoch 138/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 954.4227\n",
      "Epoch 139/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 734.0508\n",
      "Epoch 140/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 784.0646\n",
      "Epoch 141/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 966.3131 \n",
      "Epoch 142/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 839.8086\n",
      "Epoch 143/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 699.0900\n",
      "Epoch 144/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 747.5850\n",
      "Epoch 145/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 759.8093\n",
      "Epoch 146/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 688.8232\n",
      "Epoch 147/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 587.5344\n",
      "Epoch 148/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 676.5323\n",
      "Epoch 149/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 642.8647\n",
      "Epoch 150/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 594.9246\n",
      "Epoch 151/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 765.3059\n",
      "Epoch 152/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 650.8757\n",
      "Epoch 153/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 615.6442\n",
      "Epoch 154/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 701.7269\n",
      "Epoch 155/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 564.7253\n",
      "Epoch 156/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 621.1693\n",
      "Epoch 157/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 581.0931\n",
      "Epoch 158/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 615.9937\n",
      "Epoch 159/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 595.3367\n",
      "Epoch 160/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 607.7872\n",
      "Epoch 161/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 601.9565\n",
      "Epoch 162/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 603.5728\n",
      "Epoch 163/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 632.6779\n",
      "Epoch 164/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 756.7615\n",
      "Epoch 165/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 682.7767\n",
      "Epoch 166/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 747.1107\n",
      "Epoch 167/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 630.9160\n",
      "Epoch 168/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 615.5154\n",
      "Epoch 169/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 578.7064\n",
      "Epoch 170/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 597.7150\n",
      "Epoch 171/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 540.0148\n",
      "Epoch 172/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 588.6763\n",
      "Epoch 173/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 557.0313\n",
      "Epoch 174/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 578.0792\n",
      "Epoch 175/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 568.3468\n",
      "Epoch 176/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 628.2352\n",
      "Epoch 177/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 571.8647\n",
      "Epoch 178/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 583.5292\n",
      "Epoch 179/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 598.9535\n",
      "Epoch 180/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 565.2192\n",
      "Epoch 181/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 571.3061\n",
      "Epoch 182/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 518.0225\n",
      "Epoch 183/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 518.4215\n",
      "Epoch 184/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 639.0630\n",
      "Epoch 185/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 495.0078\n",
      "Epoch 186/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 479.0986\n",
      "Epoch 187/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 479.5671\n",
      "Epoch 188/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 531.3771\n",
      "Epoch 189/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 543.1321\n",
      "Epoch 190/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 545.9293\n",
      "Epoch 191/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 592.3953\n",
      "Epoch 192/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 538.8956\n",
      "Epoch 193/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 545.7454\n",
      "Epoch 194/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 502.8672\n",
      "Epoch 195/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 553.1763\n",
      "Epoch 196/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 556.3875\n",
      "Epoch 197/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 561.7314\n",
      "Epoch 198/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 727.4440\n",
      "Epoch 199/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 639.4262\n",
      "Epoch 200/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 580.0136\n",
      "Epoch 201/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 575.5981\n",
      "Epoch 202/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 484.9006\n",
      "Epoch 203/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 468.1691\n",
      "Epoch 204/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 515.0237\n",
      "Epoch 205/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 543.0973\n",
      "Epoch 206/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 504.7285\n",
      "Epoch 207/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 491.8515\n",
      "Epoch 208/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 514.5291\n",
      "Epoch 209/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 458.2853\n",
      "Epoch 210/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 506.7057\n",
      "Epoch 211/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 470.5404\n",
      "Epoch 212/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 529.6257\n",
      "Epoch 213/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 547.1422\n",
      "Epoch 214/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 489.1356\n",
      "Epoch 215/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 521.0165\n",
      "Epoch 216/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 492.4004\n",
      "Epoch 217/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 448.6242\n",
      "Epoch 218/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 489.7547\n",
      "Epoch 219/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 469.9345\n",
      "Epoch 220/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 469.2320\n",
      "Epoch 221/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 505.1740\n",
      "Epoch 222/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 440.7868\n",
      "Epoch 223/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 465.7957\n",
      "Epoch 224/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 445.4113\n",
      "Epoch 225/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 428.8836\n",
      "Epoch 226/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 477.2161\n",
      "Epoch 227/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 508.1563\n",
      "Epoch 228/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 488.0140\n",
      "Epoch 229/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 477.7158\n",
      "Epoch 230/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 484.0528\n",
      "Epoch 231/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 468.6534\n",
      "Epoch 232/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 419.1848\n",
      "Epoch 233/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 452.6726\n",
      "Epoch 234/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 490.4675\n",
      "Epoch 235/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 515.3552\n",
      "Epoch 236/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 493.9574\n",
      "Epoch 237/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 533.0455\n",
      "Epoch 238/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 465.9549\n",
      "Epoch 239/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 490.1207\n",
      "Epoch 240/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 442.1373\n",
      "Epoch 241/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 429.5663\n",
      "Epoch 242/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 456.1500\n",
      "Epoch 243/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 500.0444\n",
      "Epoch 244/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 567.6666\n",
      "Epoch 245/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 519.9997\n",
      "Epoch 246/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 471.3149\n",
      "Epoch 247/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 455.9058\n",
      "Epoch 248/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 435.3210\n",
      "Epoch 249/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 453.4594\n",
      "Epoch 250/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 497.2505\n",
      "Epoch 251/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 534.5344\n",
      "Epoch 252/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 470.9930\n",
      "Epoch 253/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 489.4562\n",
      "Epoch 254/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 457.6604\n",
      "Epoch 255/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 455.6552\n",
      "Epoch 256/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 449.8122\n",
      "Epoch 257/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 502.9945\n",
      "Epoch 258/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 423.0793\n",
      "Epoch 259/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 383.3895\n",
      "Epoch 260/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 472.4024\n",
      "Epoch 261/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 545.1469\n",
      "Epoch 262/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 580.7029\n",
      "Epoch 263/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 508.1531\n",
      "Epoch 264/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 500.3049\n",
      "Epoch 265/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 473.8795\n",
      "Epoch 266/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 433.9307\n",
      "Epoch 267/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 507.1108\n",
      "Epoch 268/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 444.1571\n",
      "Epoch 269/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 440.0690\n",
      "Epoch 270/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 429.0345\n",
      "Epoch 271/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 460.7851\n",
      "Epoch 272/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 400.8373\n",
      "Epoch 273/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 448.2863\n",
      "Epoch 274/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 472.2255\n",
      "Epoch 275/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 473.8226\n",
      "Epoch 276/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 417.8232\n",
      "Epoch 277/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 435.2098\n",
      "Epoch 278/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 426.8187\n",
      "Epoch 279/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 474.0473\n",
      "Epoch 280/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 401.2061\n",
      "Epoch 281/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 391.1714\n",
      "Epoch 282/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 414.5239\n",
      "Epoch 283/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 540.5406\n",
      "Epoch 284/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 551.6068\n",
      "Epoch 285/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 454.1346\n",
      "Epoch 286/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 422.2466\n",
      "Epoch 287/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 471.4212\n",
      "Epoch 288/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 427.4807\n",
      "Epoch 289/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 417.5460\n",
      "Epoch 290/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 413.9593\n",
      "Epoch 291/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 431.0896\n",
      "Epoch 292/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 436.2098\n",
      "Epoch 293/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 459.8623\n",
      "Epoch 294/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 412.0527\n",
      "Epoch 295/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 418.9531\n",
      "Epoch 296/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 395.9157\n",
      "Epoch 297/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 430.9827\n",
      "Epoch 298/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 429.4238\n",
      "Epoch 299/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 399.4651\n",
      "Epoch 300/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 452.6169\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#best try without feature engineering\n",
    "train = pd.read_csv(\"/Users/hwind/Downloads/Python/Master/3_Semester/Data Science/Project/ids2025_group7_project_g1/Data/train.csv\")\n",
    "test = pd.read_csv(\"/Users/hwind/Downloads/Python/Master/3_Semester/Data Science/Project/ids2025_group7_project_g1/Data/test.csv\")\n",
    "groups =[col for col in train.columns if col.startswith('Group')]\n",
    "print(train.head())\n",
    "X_train = train[groups]\n",
    "y_train = train['Tm']\n",
    "X_test = test[groups]\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train.astype(\"float32\")))\n",
    "ds_train = ds_train.shuffle(5000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)  # linear output\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=300,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Tm\": pred_test\n",
    "}).to_csv(\"/Users/hwind/Downloads/Python/Master/3_Semester/Data Science/Project/ids2025_group7_project_g1/Submissions/predictions_NN_wo_feature_engineering.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f610c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter MLP: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (64, 64), 'learning_rate': 'adaptive'}\n",
      "Bester Score: 39.12846646924251\n"
     ]
    }
   ],
   "source": [
    "#first try neural networks without the part of the lecture\n",
    "#neural network\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(64,64), (128,64)],\n",
    "    'activation': ['relu'],\n",
    "    'alpha': [0.0001, 0.001],       # Regularisierung\n",
    "    'learning_rate': ['adaptive']\n",
    "}\n",
    "\n",
    "#mlp = MLPRegressor(max_iter=300)\n",
    "mlp = MLPRegressor(max_iter=500, early_stopping=True, random_state=0)\n",
    "\n",
    "grid_mlp = GridSearchCV(mlp, param_grid_mlp, cv=2, scoring=\"neg_mean_absolute_error\",n_jobs=-1 )\n",
    "grid_mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Beste Parameter MLP:\", grid_mlp.best_params_)\n",
    "print(\"Bester Score:\", -grid_mlp.best_score_)\n",
    "\n",
    "y_pred = grid_mlp.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Tm': y_pred})\n",
    "#submission.to_csv(\"Submissions/predictions_neural_network_fast.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08348f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
