{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e5004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Computing Featuresâ€¦\n",
      "Train shape: (2662, 2256) Test shape: (666, 2256)\n",
      "\n",
      "========== LightGBM: 5-Fold CV ==========\n",
      "LightGBM Fold 1 MAE: 30.9597\n",
      "LightGBM Fold 2 MAE: 28.6413\n",
      "LightGBM Fold 3 MAE: 28.0686\n",
      "LightGBM Fold 4 MAE: 29.4618\n",
      "LightGBM Fold 5 MAE: 29.3733\n",
      ">>> LightGBM OOF MAE: 29.3013\n",
      "\n",
      "\n",
      "========== XGBoost: 5-Fold CV ==========\n",
      "XGBoost Fold 1 MAE: 28.1343\n",
      "XGBoost Fold 2 MAE: 26.6812\n",
      "XGBoost Fold 3 MAE: 27.5682\n",
      "XGBoost Fold 4 MAE: 28.0558\n",
      "XGBoost Fold 5 MAE: 26.3206\n",
      ">>> XGBoost OOF MAE: 27.3521\n",
      "\n",
      "\n",
      "========== CatBoost: 5-Fold CV ==========\n",
      "CatBoost Fold 1 MAE: 28.7246\n",
      "CatBoost Fold 2 MAE: 27.6681\n",
      "CatBoost Fold 3 MAE: 27.5868\n",
      "CatBoost Fold 4 MAE: 28.8916\n",
      "CatBoost Fold 5 MAE: 27.1770\n",
      ">>> CatBoost OOF MAE: 28.0097\n",
      "\n",
      "Simple Ensemble MAE: 27.56120000944658\n",
      "Weighted Ensemble MAE: 27.680900190589977\n",
      "\n",
      "Stacking (Ridge) MAE: 27.243908942831457\n",
      "\n",
      "âœ” Alle Submission-Dateien erfolgreich exportiert!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CLEAN ENSEMBLE PIPELINE: RDKit + LGBM + XGB + CAT + STACKING\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "\n",
    "# ML / CV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) DATA LOADING\n",
    "# ================================================================\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y = train[\"Tm\"].values\n",
    "smiles_train = train[\"SMILES\"].tolist()\n",
    "smiles_test  = test[\"SMILES\"].tolist()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) FEATURE ENGINEERING (RDKit Descriptors + Morgan FP)\n",
    "# ================================================================\n",
    "\n",
    "# Get RDKit descriptor functions\n",
    "all_desc = Descriptors._descList\n",
    "desc_names = [d[0] for d in all_desc]\n",
    "desc_funcs = [d[1] for d in all_desc]\n",
    "\n",
    "NUM_FP = 2048\n",
    "NUM_DESC = len(desc_names)\n",
    "\n",
    "def featurize(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(NUM_DESC + NUM_FP)\n",
    "\n",
    "    # RDKit Descriptors\n",
    "    desc_vals = []\n",
    "    for f in desc_funcs:\n",
    "        try:\n",
    "            v = f(mol)\n",
    "        except:\n",
    "            v = 0.0\n",
    "        if v is None or pd.isna(v):\n",
    "            v = 0.0\n",
    "        desc_vals.append(v)\n",
    "\n",
    "    desc_vals = np.array(desc_vals)\n",
    "\n",
    "    # Morgan Fingerprint\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=NUM_FP)\n",
    "    fp = np.array(fp, dtype=float)\n",
    "\n",
    "    return np.concatenate([desc_vals, fp])\n",
    "\n",
    "\n",
    "print(\"ðŸ”„ Computing Featuresâ€¦\")\n",
    "X_train = np.vstack([featurize(s) for s in smiles_train])\n",
    "X_test  = np.vstack([featurize(s) for s in smiles_test])\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) GENERIC CROSS-VALIDATION FUNCTION\n",
    "# ================================================================\n",
    "\n",
    "def run_cv(model, X, y, X_test, name, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros(len(y))\n",
    "    test_preds = []\n",
    "\n",
    "    print(f\"\\n========== {name}: {n_splits}-Fold CV ==========\")\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        # ----- LightGBM -----\n",
    "        if isinstance(model, lgb.LGBMRegressor):\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.log_evaluation(period=0)]\n",
    "            )\n",
    "\n",
    "        # ----- XGBoost -----\n",
    "        elif isinstance(model, xgb.XGBRegressor):\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False      # wichtig: XGB akzeptiert verbose\n",
    "            )\n",
    "\n",
    "        # ----- CatBoost -----\n",
    "        elif isinstance(model, CatBoostRegressor):\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type:\", model)\n",
    "\n",
    "        # ----- Predictions -----\n",
    "        pred = model.predict(X_val)\n",
    "        fold_mae = mean_absolute_error(y_val, pred)\n",
    "        print(f\"{name} Fold {fold} MAE: {fold_mae:.4f}\")\n",
    "\n",
    "        oof[val_idx] = pred\n",
    "        test_preds.append(model.predict(X_test))\n",
    "\n",
    "    # Final CV result\n",
    "    oof_mae = mean_absolute_error(y, oof)\n",
    "    test_mean = np.mean(test_preds, axis=0)\n",
    "\n",
    "    print(f\">>> {name} OOF MAE: {oof_mae:.4f}\\n\")\n",
    "    return oof, test_mean, oof_mae\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4) MODEL DEFINITIONS\n",
    "# ================================================================\n",
    "\n",
    "def build_lgbm():\n",
    "    return lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        metric=\"l1\",\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=48,\n",
    "        feature_fraction=0.85,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=3,\n",
    "        min_data_in_leaf=50,\n",
    "        lambda_l1=1.0,\n",
    "        lambda_l2=1.0,\n",
    "        n_estimators=2000,\n",
    "        verbosity=-1,\n",
    "    )\n",
    "\n",
    "def build_xgb():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        n_estimators=1500,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",\n",
    "        gamma=0.0,\n",
    "    )\n",
    "\n",
    "def build_cat():\n",
    "    return CatBoostRegressor(\n",
    "        loss_function=\"MAE\",\n",
    "        learning_rate=0.03,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=5.0,\n",
    "        iterations=1500,\n",
    "        verbose=False,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5) TRAIN INDIVIDUAL MODELS\n",
    "# ================================================================\n",
    "\n",
    "oof_lgb, test_lgb, mae_lgb = run_cv(build_lgbm(), X_train, y, X_test, \"LightGBM\")\n",
    "oof_xgb, test_xgb, mae_xgb = run_cv(build_xgb(), X_train, y, X_test, \"XGBoost\")\n",
    "oof_cat, test_cat, mae_cat = run_cv(build_cat(), X_train, y, X_test, \"CatBoost\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) ENSEMBLES\n",
    "# ================================================================\n",
    "\n",
    "# Simple average ensemble\n",
    "oof_ens_simple = (oof_lgb + oof_xgb + oof_cat) / 3\n",
    "test_ens_simple = (test_lgb + test_xgb + test_cat) / 3\n",
    "mae_ens_simple = mean_absolute_error(y, oof_ens_simple)\n",
    "\n",
    "print(\"Simple Ensemble MAE:\", mae_ens_simple)\n",
    "\n",
    "# Weighted ensemble\n",
    "oof_ens_weighted = 0.4 * oof_lgb + 0.3 * oof_xgb + 0.3 * oof_cat\n",
    "test_ens_weighted = 0.4 * test_lgb + 0.3 * test_xgb + 0.3 * test_cat\n",
    "mae_ens_weighted = mean_absolute_error(y, oof_ens_weighted)\n",
    "\n",
    "print(\"Weighted Ensemble MAE:\", mae_ens_weighted)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 7) STACKING (LEVEL-2 RIDGE)\n",
    "# ================================================================\n",
    "\n",
    "X_stack = np.vstack([oof_lgb, oof_xgb, oof_cat]).T\n",
    "X_stack_test = np.vstack([test_lgb, test_xgb, test_cat]).T\n",
    "\n",
    "stacker = Ridge(alpha=0.1)\n",
    "stacker.fit(X_stack, y)\n",
    "\n",
    "oof_stack = stacker.predict(X_stack)\n",
    "test_stack = stacker.predict(X_stack_test)\n",
    "\n",
    "mae_stack = mean_absolute_error(y, oof_stack)\n",
    "print(\"\\nStacking (Ridge) MAE:\", mae_stack)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 8) EXPORT SUBMISSIONS\n",
    "# ================================================================\n",
    "\n",
    "sample = pd.read_csv(\"Submissions/sample_submission.csv\")\n",
    "\n",
    "# Individual models\n",
    "sample.assign(Tm=test_lgb) .to_csv(\"Submissions/submission_lgbm_rdkit.csv\", index=False)\n",
    "sample.assign(Tm=test_xgb) .to_csv(\"Submissions/submission_xgb_rdkit.csv\", index=False)\n",
    "sample.assign(Tm=test_cat) .to_csv(\"Submissions/submission_cat_rdkit.csv\", index=False)\n",
    "\n",
    "# Ensembles\n",
    "sample.assign(Tm=test_ens_simple)   .to_csv(\"Submissions/submission_ens_simple_rdkit.csv\", index=False)\n",
    "sample.assign(Tm=test_ens_weighted) .to_csv(\"Submissions/submission_ens_weighted_rdkit.csv\", index=False)\n",
    "\n",
    "# Stacking\n",
    "sample.assign(Tm=test_stack) .to_csv(\"Submissions/submission_stack_ridge.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ” Alle Submission-Dateien erfolgreich exportiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db7ab0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Computing Features...\n",
      "Train shape: (2662, 2256) Test shape: (666, 2256)\n",
      "\n",
      "========== XGBOOST 5-Fold CV ==========\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold MAE: 27.5113\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold MAE: 26.8771\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold MAE: 27.4852\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold MAE: 27.2596\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold MAE: 26.1409\n",
      "\n",
      ">>> FINAL XGB OOF MAE: 27.0549\n",
      "\n",
      "âœ” Saved: Submissions/submission_xgb_rdkit.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# XGBOOST â€“ CLEAN STANDALONE PIPELINE FOR MELTING-POINT PREDICTION\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) LOAD DATA\n",
    "# ================================================================\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y = train[\"Tm\"].values\n",
    "smiles_train = train[\"SMILES\"].tolist()\n",
    "smiles_test  = test[\"SMILES\"].tolist()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) FEATURE ENGINEERING\n",
    "# ================================================================\n",
    "\n",
    "# RDKit descriptors\n",
    "all_desc = Descriptors._descList\n",
    "desc_names = [d[0] for d in all_desc]\n",
    "desc_funcs = [d[1] for d in all_desc]\n",
    "\n",
    "NUM_DESC = len(desc_names)\n",
    "NUM_FP   = 2048  # Morgan fingerprint size\n",
    "\n",
    "\n",
    "def featurize(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(NUM_DESC + NUM_FP)\n",
    "\n",
    "    # RDKit descriptors\n",
    "    desc_values = []\n",
    "    for f in desc_funcs:\n",
    "        try:\n",
    "            v = f(mol)\n",
    "        except:\n",
    "            v = 0.0\n",
    "        if v is None or pd.isna(v):\n",
    "            v = 0.0\n",
    "        desc_values.append(v)\n",
    "\n",
    "    desc_values = np.array(desc_values)\n",
    "\n",
    "    # Morgan fingerprint\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=NUM_FP)\n",
    "    fp = np.array(fp, dtype=float)\n",
    "\n",
    "    return np.concatenate([desc_values, fp])\n",
    "\n",
    "\n",
    "print(\"ðŸ”„ Computing Features...\")\n",
    "X_train = np.vstack([featurize(s) for s in smiles_train])\n",
    "X_test  = np.vstack([featurize(s) for s in smiles_test])\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) XGBOOST MODEL\n",
    "# ================================================================\n",
    "\n",
    "def build_xgb():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        n_estimators=8000,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        min_child_weight=5,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=2.0,\n",
    "        tree_method=\"hist\",\n",
    "        gamma=0.0,\n",
    "    )\n",
    "\n",
    "#### RESULTS: \n",
    "# default params: \n",
    "# objective=\"reg:squarederror\", learning_rate=0.03, max_depth=6, \n",
    "# subsample=0.85, colsample_bytree=0.85, n_estimators=8000, reg_alpha=0.3, reg_lambda=1.0,\n",
    "# tree_method=\"hist\", gamma=0.0,\n",
    "\n",
    "# TEST1: max_depth = 7 & n_estimators = 1500 -> MAE: 27.3521\n",
    "# TEST2: max_depth = 6 & n_estimators = 8000 -> MAE: 26.9902\n",
    "# TEST3: max_depth = 6 & n_estimators = 8000 & gamma=1.0 -> MAE: 26.9620 (but in Kaggle worse than TEST2)\n",
    "# TEST4: learning_rate=0.02 & max_depth = 6 & n_estimators = 8000 & reg_alpha=1.0, & reg_lambda=2.0 -> MAE: \n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4) CROSS-VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "def run_cv_xgb(X, y, X_test, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    oof = np.zeros(len(y))\n",
    "    test_folds = []\n",
    "\n",
    "    print(f\"\\n========== XGBOOST {n_splits}-Fold CV ==========\")\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
    "\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = build_xgb()\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        pred_val = model.predict(X_val)\n",
    "        fold_mae = mean_absolute_error(y_val, pred_val)\n",
    "\n",
    "        print(f\"Fold MAE: {fold_mae:.4f}\")\n",
    "\n",
    "        oof[val_idx] = pred_val\n",
    "        test_folds.append(model.predict(X_test))\n",
    "\n",
    "    oof_mae = mean_absolute_error(y, oof)\n",
    "    test_pred = np.mean(test_folds, axis=0)\n",
    "\n",
    "    print(f\"\\n>>> FINAL XGB OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "    return oof, test_pred, oof_mae\n",
    "\n",
    "\n",
    "# Run CV\n",
    "oof_xgb, test_xgb, mae_xgb = run_cv_xgb(X_train, y, X_test)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5) EXPORT SUBMISSION\n",
    "# ================================================================\n",
    "\n",
    "sample = pd.read_csv(\"Submissions/sample_submission.csv\")\n",
    "sample[\"Tm\"] = test_xgb\n",
    "sample.to_csv(\"Submissions/submission_xgb_rdkit.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ” Saved: Submissions/submission_xgb_rdkit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef7ea859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 2662\n",
      "Test rows : 666\n",
      "Computing RDKit features... (this can take a few minutes)\n",
      "Shapes:\n",
      " â†’ Train features: (2662, 2256)\n",
      " â†’ Test features : (666, 2256)\n",
      "\n",
      "âœ“ Saved train_features.csv\n",
      "âœ“ Saved test_features.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "# =====================================================\n",
    "# 1) Load Data\n",
    "# =====================================================\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "smiles_train = train[\"SMILES\"].tolist()\n",
    "smiles_test  = test[\"SMILES\"].tolist()\n",
    "\n",
    "print(\"Train rows:\", len(train))\n",
    "print(\"Test rows :\", len(test))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2) Build Feature Extractor\n",
    "# =====================================================\n",
    "\n",
    "# List of RDKit descriptor functions\n",
    "all_desc = Descriptors._descList\n",
    "desc_names = [d[0] for d in all_desc]\n",
    "desc_funcs = [d[1] for d in all_desc]\n",
    "\n",
    "def featurize_smiles(smi):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string into:\n",
    "    - RDKit descriptors\n",
    "    - Morgan fingerprint (2048 bits)\n",
    "    \"\"\"\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        # Molecule invalid â†’ return zero vector\n",
    "        return np.zeros(len(desc_names) + 2048, dtype=float)\n",
    "\n",
    "    # --- RDKit Descriptors ---\n",
    "    desc_values = []\n",
    "    for func in desc_funcs:\n",
    "        try:\n",
    "            val = func(mol)\n",
    "        except:\n",
    "            val = np.nan\n",
    "        desc_values.append(val)\n",
    "\n",
    "    desc_values = np.nan_to_num(np.array(desc_values, dtype=float))\n",
    "\n",
    "    # --- Morgan Fingerprint ---\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    fp = np.array(fp, dtype=float)\n",
    "\n",
    "    return np.concatenate([desc_values, fp])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3) Compute Features\n",
    "# =====================================================\n",
    "\n",
    "print(\"Computing RDKit features... (this can take a few minutes)\")\n",
    "X_train = np.vstack([featurize_smiles(s) for s in smiles_train])\n",
    "X_test  = np.vstack([featurize_smiles(s) for s in smiles_test])\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\" â†’ Train features:\", X_train.shape)\n",
    "print(\" â†’ Test features :\", X_test.shape)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) Build Feature DataFrames\n",
    "# =====================================================\n",
    "\n",
    "# Column names\n",
    "fp_names = [f\"fp_{i}\" for i in range(2048)]\n",
    "feature_columns = desc_names + fp_names\n",
    "\n",
    "# Training DF with Tm\n",
    "train_features = pd.DataFrame(X_train, columns=feature_columns)\n",
    "train_features[\"Tm\"] = train[\"Tm\"].values   # Add target\n",
    "\n",
    "# Test DF with id\n",
    "test_features = pd.DataFrame(X_test, columns=feature_columns)\n",
    "test_features[\"id\"] = test[\"id\"].values\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) Save Feature Files\n",
    "# =====================================================\n",
    "\n",
    "train_features.to_csv(\"train_features.csv\", index=False)\n",
    "test_features.to_csv(\"test_features.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ“ Saved train_features.csv\")\n",
    "print(\"âœ“ Saved test_features.csv\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
