{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c6679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T16:34:57.815304Z",
     "iopub.status.busy": "2025-11-21T16:34:57.815070Z",
     "iopub.status.idle": "2025-11-21T18:41:08.967439Z",
     "shell.execute_reply": "2025-11-21T18:41:08.966781Z"
    },
    "papermill": {
     "duration": 7571.157014,
     "end_time": "2025-11-21T18:41:08.969338",
     "exception": false,
     "start_time": "2025-11-21T16:34:57.812324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture -> decommend if you dont want to see outputs\n",
    "%pip install rdkit\n",
    "# ==========================================================================================\n",
    "# =================================== IMPORTS AND SETUP ====================================\n",
    "# ==========================================================================================\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, MACCSkeys, RDKFingerprint, rdFingerprintGenerator\n",
    "from rdkit.Chem.AtomPairs import Pairs, Torsions\n",
    "\n",
    "# DISABLE WARNING FROM rdkit\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "try:\n",
    "    from rdkit.Avalon import pyAvalonTools\n",
    "    avalon_available = True\n",
    "except ImportError:\n",
    "    avalon_available = False\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import xgboost\n",
    "\n",
    "# ==========================================================================================\n",
    "# ==================================== LOADING DATASETS ====================================\n",
    "# ==========================================================================================\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")\n",
    "bradley_df = pd.read_excel(\"helper/BradleyMeltingPointDataset.xlsx\")\n",
    "bradleyplus_df = pd.read_excel(\"helper/BradleyDoublePlusGoodMeltingPointDataset.xlsx\")\n",
    "\n",
    "\n",
    "train_df = train_df[['SMILES', 'Tm']]\n",
    "test_df  = test_df[['id', 'SMILES']]\n",
    "\n",
    "train_df.shape, test_df.shape, bradley_df.shape, bradleyplus_df.shape\n",
    "\n",
    "display(train_df)\n",
    "display(test_df)\n",
    "\n",
    "# ==========================================================================================\n",
    "# ========================== PREPROCESSING EXTERNAL DATA (BRADLEY) =========================\n",
    "# ==========================================================================================\n",
    "\n",
    "# CONVERT MELTING POINT CELCIUS TO KELVIN (Tm)\n",
    "bradley_df['Tm'] = bradley_df['mpC'].map(lambda x : x + 273.15)\n",
    "bradleyplus_df['Tm'] = bradleyplus_df['mpC'].map(lambda x: x + 273.15)\n",
    "\n",
    "# GET ONLY SMILES AND Tm COLUMNS\n",
    "bradley_df = bradley_df[['smiles', 'Tm']]\n",
    "bradleyplus_df = bradleyplus_df[['smiles', 'Tm']]\n",
    "\n",
    "# MERGE THEM\n",
    "bradley_merge = pd.concat((bradley_df, bradleyplus_df), axis = 0)\n",
    "bradley_merge = bradley_merge.rename(columns = {'smiles' : 'SMILES'})\n",
    "\n",
    "bradley_merge\n",
    "\n",
    "# ==========================================================================================\n",
    "# =============================== MERGING AND CLEANING DATA ================================\n",
    "# ==========================================================================================\n",
    "\n",
    "# CONCAT TRAIN DATA AND EXTRA DATA\n",
    "merge_df = pd.concat((train_df, bradley_merge), axis = 0)\n",
    "\n",
    "merge_df\n",
    "\n",
    "# DROP DUPLICATED FROM MERGED DATA\n",
    "display(f'There are {merge_df.duplicated(subset = [\"SMILES\", \"Tm\"]).sum()} Duplicated data')\n",
    "\n",
    "merge_df = merge_df.drop_duplicates(subset = ['SMILES', 'Tm']).reset_index(drop = True)\n",
    "\n",
    "print(f'Successfully Drop Duplicated data!')\n",
    "\n",
    "merge_df\n",
    "\n",
    "# ==========================================================================================\n",
    "# =========================== FEATURE ENGINEERING: DESCRIPTORS =============================\n",
    "# ==========================================================================================\n",
    "\n",
    "# EXTRACT ALL DESCRIPTORS FEATURES\n",
    "def extract_all_descriptors(df, SMILES):\n",
    "\n",
    "    # GET ALL DESCRIPTORS\n",
    "    descriptor_list = Descriptors._descList    # --> THESE WILL RETURN LIST OF TUPLE\n",
    "    descriptors = [desc[0] for desc in descriptor_list]\n",
    "\n",
    "    print(f'There Are {len(descriptor_list)} Descriptor Features')\n",
    "\n",
    "    # EXTRACT ALL DESCRIPTORS FROM SMILES FEATURES\n",
    "    result = []\n",
    "    for smi in df[SMILES]:\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        # IF MOLECOLE IS INVALID\n",
    "        if mol is None:\n",
    "            row = {name : None for name, func in descriptor_list}\n",
    "        else:\n",
    "            # CREATE DESCRIPTORS FEATURES\n",
    "            row = {name: func(mol) for name, func in descriptor_list}\n",
    "\n",
    "        result.append(row)\n",
    "\n",
    "    # MERGE DATA WITH EXTRACTED FEATURES\n",
    "    df_descriptor = pd.DataFrame(result)\n",
    "    df_result = pd.concat((df, df_descriptor), axis = 1)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "merge_df = extract_all_descriptors(merge_df, 'SMILES')\n",
    "test_df  = extract_all_descriptors(test_df, 'SMILES')\n",
    "\n",
    "# DROP NULL/NONE VALUE AFTER FEATURE ENGINEERING\n",
    "merge_df = merge_df.dropna().reset_index(drop = True)\n",
    "test_df = test_df.dropna().reset_index(drop = True)\n",
    "\n",
    "merge_df\n",
    "\n",
    "# ==========================================================================================\n",
    "# =========================== FEATURE ENGINEERING: FINGERPRINTS ============================\n",
    "# ==========================================================================================\n",
    "\n",
    "# EXTRACT ALL MOLECULAR FINGERPRINT FEATURES\n",
    "def extract_all_fingerprint(df, SMILES, morgan_radius = 2, morgan_nbits = 1024):\n",
    "\n",
    "    fps_data = []  # --> STORE NEW FEATURES DATA\n",
    "\n",
    "    # DEFINE MORGAN GENERATOR\n",
    "    morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius = morgan_radius, fpSize = morgan_nbits, countSimulation = True, includeChirality = False)\n",
    "\n",
    "    fcfp = rdFingerprintGenerator.GetMorganFeatureAtomInvGen()\n",
    "    fcfp_gen = rdFingerprintGenerator.GetMorganGenerator(radius = morgan_nbits, fpSize = morgan_nbits, atomInvariantsGenerator = fcfp, countSimulation= True, includeChirality = False)\n",
    "\n",
    "    atom_gen = rdFingerprintGenerator.GetAtomPairGenerator(fpSize = 2048, countSimulation= True, includeChirality = False)\n",
    "\n",
    "    # ITERATE EVERY SAMPLE OF SMILES FEATURES\n",
    "    for smiles in df[SMILES]:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        if mol is None:\n",
    "            print(smiles, 'is Invalid!')\n",
    "            fps_data.append({})\n",
    "            continue\n",
    "\n",
    "        # STORE NEW FEATURE FOR EACH SAMPLES CREATED\n",
    "        feature_rows = {}\n",
    "\n",
    "        # MORGAN FINGERPRINT (ECFP)\n",
    "        morgan_fp = morgan_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_rows[f\"Morgan_{i}\"] = morgan_fp[i]\n",
    "\n",
    "        # FUNCTIONAL-CLASS FINGERPRINT (FCFP)\n",
    "        fc_fp = fcfp_gen.GetFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_rows[f\"FCFP_{i}\"] = fc_fp[i]\n",
    "\n",
    "        # MACCS KEYS (166 BITS)\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        for i in range(len(maccs_fp)):\n",
    "            feature_rows[f\"MACCS_{i}\"] = int(maccs_fp[i])\n",
    "\n",
    "        # AtomPair Fingerprint (2D)\n",
    "        atompair_fp = atom_gen.GetCountFingerprint(mol)\n",
    "        for i in range(morgan_nbits):\n",
    "            feature_rows[f\"AtomPair_{i}\"] = atompair_fp[i]\n",
    "\n",
    "        # RDKIT FINGERPRINT\n",
    "        rdkit_fp = RDKFingerprint(mol)\n",
    "        for i in range(len(rdkit_fp)):\n",
    "            feature_rows[f\"RDKIT_{i}\"] = int(rdkit_fp[i])\n",
    "\n",
    "        # AVALON FINGERPRINT (IF AVAILABLE) \n",
    "        if avalon_available:\n",
    "            avalon_fp = pyAvalonTools.GetAvalonFP(mol, morgan_nbits)\n",
    "        for i in range(len(avalon_fp)):\n",
    "            feature_rows[f\"Avalon_{i}\"] = int(avalon_fp[i])\n",
    "\n",
    "\n",
    "        fps_data.append(feature_rows)\n",
    "\n",
    "    print(f'There are {morgan_nbits} Morgan Fingerprint Features')\n",
    "    print(f'There are {len(maccs_fp)} MACCS Keys Features')\n",
    "    print(f'There are {len(rdkit_fp)} RDKIT Fingerprint Features')\n",
    "\n",
    "    # MERGE REAL DATA WITH EXTRACTED FEATURES\n",
    "    fps_df = pd.DataFrame(fps_data)\n",
    "    df_result = pd.concat((df, fps_df), axis = 1)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# APPLY FUNCTION\n",
    "merge_df = extract_all_fingerprint(merge_df, 'SMILES')\n",
    "test_df  = extract_all_fingerprint(test_df, 'SMILES')\n",
    "\n",
    "merge_df\n",
    "\n",
    "# ==========================================================================================\n",
    "# ==================================== DATA SPLITTING ======================================\n",
    "# ==========================================================================================\n",
    "\n",
    "# SPLIT DATA\n",
    "x = merge_df.drop(labels = ['SMILES', 'Tm'], axis = 1)\n",
    "y = merge_df['Tm']\n",
    "\n",
    "x_test = test_df.drop(labels = ['SMILES', 'id'], axis = 1)\n",
    "\n",
    "x.shape, y.shape, x_test.shape, type(x)\n",
    "\n",
    "# ==========================================================================================\n",
    "# =========================== OPTUNA HYPERPARAMETER OPTIMIZATION ===========================\n",
    "# ==========================================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    xgb_params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", ['reg:squarederror', 'reg:pseudohubererror']),\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        'predictor' : 'gpu_predictor',\n",
    "        'device' : 'cuda',\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        'n_estimators' : 10_000,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 3e-3, 0.3, log=True),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.1, 20.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 20.0, log=True),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state = 2025)\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(x):\n",
    "        X_train, X_valid = x.iloc[train_idx], x.iloc[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "        dvalid = xgboost.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "        model = xgboost.train(\n",
    "            xgb_params,\n",
    "            dtrain,\n",
    "            num_boost_round=10000,\n",
    "            evals=[(dvalid, \"validation\")],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(dvalid)\n",
    "        rmse = mean_squared_error(y_valid, preds, squared = False)\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# START OPTUNA\n",
    "\n",
    "# MOVE AND COPY FILE TO OUPUT\n",
    "shutil.copy(src = '/kaggle/input/optuna-study-3-models/other/optuna-study-3-models/2/xgb_study.db', \n",
    "            dst = '/kaggle/working/xgb_study.db')\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name = 'xgb_study', \n",
    "                            storage=\"sqlite://////kaggle/working/xgb_study.db\", \n",
    "                            load_if_exists = True)\n",
    "\n",
    "# CLEAR OUTPUT AFTER TRAINING \n",
    "clear_output(wait=True)\n",
    "\n",
    "print(f'Training Complete! Congrats!')\n",
    "print(f'Total Number of Trials : {len(study.trials)}\\n')\n",
    "\n",
    "print(\"Best Trial\", study.best_trial.number)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "print(\"Best Params:\", study.best_trial.params)\n",
    "\n",
    "# ==========================================================================================\n",
    "# ============================ MODEL CONFIGURATION AND PARAMS ==============================\n",
    "# ==========================================================================================\n",
    "\n",
    "# TRAIN XGB WITH BEST PARAMETERS\n",
    "\n",
    "#best_params = study.best_trial.params\n",
    "#best_params.update({'eval_metric' : \"mae\",\n",
    "#                    'device' : 'cpu'})\n",
    "\n",
    "# USE SIMPLER MODEL THAN TUNED MODEL (DUE TO SLOW COMPUTATION)\n",
    "best_params = {\n",
    "    'max_depth' : 6,\n",
    "    'eta' : 0.1,\n",
    "    'tree_method' : 'hist',\n",
    "    'eval_metric' : 'mae'\n",
    "}\n",
    "\n",
    "best_params\n",
    "\n",
    "'''{'objective': 'reg:squarederror',\n",
    " 'max_depth': 3,\n",
    " 'learning_rate': 0.01688021212211354,\n",
    " 'min_child_weight': 5,\n",
    " 'subsample': 0.8943127227676447,\n",
    " 'colsample_bytree': 0.590582609011384,\n",
    " 'gamma': 0.01132850232872052,\n",
    " 'lambda': 4.445806747037075,\n",
    " 'alpha': 0.1292033669407927,\n",
    " 'eval_metric': 'mae',\n",
    " 'device': 'cpu'}'''\n",
    "\n",
    "# ==========================================================================================\n",
    "# ============================== CROSS-VALIDATION TRAINING =================================\n",
    "# ==========================================================================================\n",
    "\n",
    "# XGB WITH SKFOLD\n",
    "\n",
    "skfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 2025)\n",
    "\n",
    "# DEFINE YEO-JOHNSON FOR TRANSFORMING TARGET FEATURE INSIDE LOOP\n",
    "yeo = PowerTransformer(method = 'yeo-johnson')\n",
    "\n",
    "# STORE OOF AND TEST MAE\n",
    "oof_val = np.zeros(len(x))\n",
    "train_score , val_score, test_pred = [], [], []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skfold.split(x, pd.qcut(y, q = 10).cat.codes)):\n",
    "    \n",
    "    # SPLIT DATA\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # USING YEO-JOHNSON TO TRANSFORM TARGET FEATURE\n",
    "    y_train = yeo.fit_transform(y_train.values.reshape(-1, 1)).squeeze()\n",
    "    y_val   = yeo.transform(y_val.values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "    # BUILD TINY CATBOOST TO GET THE MOST IMPORTANT FEATURE ONLY\n",
    "    selector_model = xgboost.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth = 6,\n",
    "        learning_rate=0.05,\n",
    "        random_state=2025,\n",
    "        device = 'cpu',\n",
    "        objective=\"reg:absoluteerror\",\n",
    "        tree_method = 'hist',\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # GET THE MOST IMPORTANT FEATURE ONLY \n",
    "    selector_model.fit(x_train, y_train)\n",
    "    selector = SelectFromModel(selector_model, prefit=True, threshold=\"mean\")\n",
    "\n",
    "    # CHECK AND DISPLAY HOW MANY IMPORTANT FEATURES ARE\n",
    "    selected_idx = selector.get_support(indices=True) \n",
    "    selected_features = x_train.columns[selected_idx]\n",
    "    print(\"Selected features:\", len(selected_features),\"\\n\")\n",
    "\n",
    "    # APPLY TRANSFORM (ONLY GET MOST IMPORTANT FEATURE AND REMOVE USELESS FEATURES)\n",
    "    x_train_new = x_train[selected_features]\n",
    "    x_val_new   = x_val[selected_features]\n",
    "    x_test_new  = x_test[selected_features]\n",
    "\n",
    "    # XGBOOST DATASET\n",
    "    dtrain = xgboost.DMatrix(x_train_new, y_train, feature_names = selected_features.tolist())\n",
    "    dval   = xgboost.DMatrix(x_val_new, y_val, feature_names = selected_features.tolist())\n",
    "    dtest  = xgboost.DMatrix(x_test_new, feature_names = selected_features.tolist())\n",
    "\n",
    "    # XGBOOST\n",
    "    xgb = xgboost.train(params = best_params, \n",
    "                        dtrain = dtrain, \n",
    "                        num_boost_round = 15_000, \n",
    "                        evals = [(dtrain, 'train'), (dval, 'valid')],\n",
    "                        early_stopping_rounds = 300,\n",
    "                        verbose_eval = 1000)\n",
    "    \n",
    "    # DISPLAY SELECTED IMPORTANCE FEATURES\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    xgboost.plot_importance(booster = xgb, importance_type = 'gain', max_num_features = 30, ax = ax)\n",
    "    plt.title(f'Feature Importance fold {i+1}')\n",
    "    plt.show()\n",
    "\n",
    "    # PREDICT\n",
    "    y_train_predict = xgb.predict(dtrain)\n",
    "    y_val_predict   = xgb.predict(dval)\n",
    "    y_test_predict = xgb.predict(dtest)\n",
    "\n",
    "    # INVERSE TRANSFORM (TRANSFORM A VALUE BACK TO NORMAL)\n",
    "    y_train = yeo.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
    "    y_val   = yeo.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
    "    y_train_predict = yeo.inverse_transform(y_train_predict.reshape(-1, 1)).squeeze()\n",
    "    y_val_predict   = yeo.inverse_transform(y_val_predict.reshape(-1, 1)).squeeze()\n",
    "    y_test_predict  = yeo.inverse_transform(y_test_predict.reshape(-1, 1)).squeeze()\n",
    "\n",
    "    # MAE\n",
    "    train_mae = mean_absolute_error(y_train, y_train_predict)\n",
    "    val_mae   = mean_absolute_error(y_val, y_val_predict)\n",
    "\n",
    "    print(f'Fold {i+1} : Train MAE = {train_mae}, Val MAE = {val_mae}')\n",
    "\n",
    "    train_score.append(train_mae)\n",
    "    val_score.append(val_mae)\n",
    "\n",
    "    # PUSH OOF PREDICTION AND TEST PREDICTION\n",
    "    oof_val[val_index] = y_val_predict\n",
    "    test_pred.append(y_test_predict)\n",
    "\n",
    "\n",
    "print(f'\\nTrain Fold Prediction : {np.mean(train_score)}')\n",
    "print(f'Val Fold Prediction   : {np.mean(val_score)}\\n')\n",
    "\n",
    "print(f'std Train Fold Prediction : {np.std(train_score, ddof = 0)}')\n",
    "print(f'std Val Fold Prediction   : {np.std(val_score, ddof = 0)}')\n",
    "\n",
    "# ==========================================================================================\n",
    "# ============================== EVALUATION AND VISUALIZATION ==============================\n",
    "# ==========================================================================================\n",
    "\n",
    "# ACTUAL VS PREDICTED LABEL\n",
    "\n",
    "y_true = merge_df['Tm'].values\n",
    "y_pred = oof_val\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# SCATTER PLOT 2D\n",
    "plt.scatter(y_pred, y_true, alpha=0.7)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.title('Actual vs Predicted')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================================================\n",
    "# ============================== OPTUNA PLOTS AND ANALYSIS =================================\n",
    "# ==========================================================================================\n",
    "\n",
    "# SHOW OPTIMIZATION HISTORY \n",
    "optuna.visualization.plot_optimization_history(study) # --> SHOW THE BEST VALUE LINE DURING STUDY\n",
    "\n",
    "# ANALYZE HYPERPARAMETER INTERACTIONS\n",
    "optuna.visualization.plot_parallel_coordinate(study)\n",
    "\n",
    "# DISPLAY SENSITIVY EACH PARAMETER\n",
    "optuna.visualization.plot_slice(study)\n",
    "\n",
    "# DISPLAY DECISION BOUNDARY \n",
    "fig = optuna.visualization.plot_contour(study, params=['alpha',\n",
    "                                                 'colsample_bytree',\n",
    "                                                 'gamma',\n",
    "                                                 'lambda',\n",
    "                                                 'learning_rate',\n",
    "                                                 'max_depth'])\n",
    "fig.update_layout(width = 2000, height = 900)\n",
    "\n",
    "# EMPIRICAL DISTRIBUTION\n",
    "optuna.visualization.plot_edf(study) # DISPLAY PROBABILITY OF OBJECTIVE VALUE IS LESS THAN OR EQUAL TO A GIVEN THRESHOLD (EDF CURVE)\n",
    "\n",
    "# DISPLAY HYPERPARAMETER IMPORTANCES\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "\n",
    "# ==========================================================================================\n",
    "# ====================================== SUBMISSION ========================================\n",
    "# ==========================================================================================\n",
    "\n",
    "# SUBMISSION\n",
    "\n",
    "submission = pd.read_csv(r'/kaggle/input/melting-point/sample_submission.csv')\n",
    "\n",
    "submission['Tm'] = np.mean(test_pred, axis = 0)\n",
    "\n",
    "submission\n",
    "\n",
    "# SAVE SUBMISSION\n",
    "submission.to_csv(r'submission_xgb_176460.csv', index = False)\n",
    "\n",
    "# SAVE OOF PREDICTION\n",
    "\n",
    "oof_df = pd.DataFrame(oof_val) # --> CONVERT TO DATAFRAME\n",
    "\n",
    "display(oof_df)\n",
    "\n",
    "# SAVE OOF\n",
    "oof_df.to_csv(r'oof_xgb_176460.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13473948,
     "sourceId": 113155,
     "sourceType": "competition"
    },
    {
     "datasetId": 8364474,
     "sourceId": 13248875,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 456306,
     "modelInstanceId": 439753,
     "sourceId": 588401,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7576.800219,
   "end_time": "2025-11-21T18:41:10.100645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T16:34:53.300426",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
