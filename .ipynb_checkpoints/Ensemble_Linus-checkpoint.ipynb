{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ae5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linus\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9d840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa5980c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM | Fold 1 / 5 =====\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1994]\tvalid_0's l1: 30.9591\n",
      "LightGBM Fold 1 MAE: 30.9591\n",
      "\n",
      "===== LightGBM | Fold 2 / 5 =====\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 28.6413\n",
      "LightGBM Fold 2 MAE: 28.6413\n",
      "\n",
      "===== LightGBM | Fold 3 / 5 =====\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1452]\tvalid_0's l1: 28.1396\n",
      "LightGBM Fold 3 MAE: 28.1396\n",
      "\n",
      "===== LightGBM | Fold 4 / 5 =====\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's l1: 29.4617\n",
      "LightGBM Fold 4 MAE: 29.4617\n",
      "\n",
      "===== LightGBM | Fold 5 / 5 =====\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 29.3733\n",
      "LightGBM Fold 5 MAE: 29.3733\n",
      "\n",
      ">>> LightGBM OOF MAE: 29.3154\n",
      "\n",
      "===== XGBoost | Fold 1 / 5 =====\n",
      "XGBoost Fold 1 MAE: 28.1343\n",
      "\n",
      "===== XGBoost | Fold 2 / 5 =====\n",
      "XGBoost Fold 2 MAE: 26.6812\n",
      "\n",
      "===== XGBoost | Fold 3 / 5 =====\n",
      "XGBoost Fold 3 MAE: 27.5682\n",
      "\n",
      "===== XGBoost | Fold 4 / 5 =====\n",
      "XGBoost Fold 4 MAE: 28.0558\n",
      "\n",
      "===== XGBoost | Fold 5 / 5 =====\n",
      "XGBoost Fold 5 MAE: 26.3206\n",
      "\n",
      ">>> XGBoost OOF MAE: 27.3521\n",
      "\n",
      "===== CatBoost | Fold 1 / 5 =====\n",
      "CatBoost Fold 1 MAE: 28.6019\n",
      "\n",
      "===== CatBoost | Fold 2 / 5 =====\n",
      "CatBoost Fold 2 MAE: 27.5939\n",
      "\n",
      "===== CatBoost | Fold 3 / 5 =====\n",
      "CatBoost Fold 3 MAE: 27.9625\n",
      "\n",
      "===== CatBoost | Fold 4 / 5 =====\n",
      "CatBoost Fold 4 MAE: 27.9421\n",
      "\n",
      "===== CatBoost | Fold 5 / 5 =====\n",
      "CatBoost Fold 5 MAE: 27.3970\n",
      "\n",
      ">>> CatBoost OOF MAE: 27.8996\n",
      "\n",
      "Ensemble (simple average) OOF MAE: 27.55732563370291\n",
      "Ensemble (weighted) OOF MAE: 27.680140042861918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-30 13:43:04,667] Trial 0 failed with parameters: {'lr': 0.048450228693825016, 'leaves': 91, 'max_depth': 10, 'ff': 0.8758074557214812, 'bf': 0.9062005131143853, 'bfreq': 4, 'minleaf': 40, 'l1': 6.626885735826981, 'l2': 6.949893988149936e-05} because of the following error: TypeError(\"LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\linus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\linus\\AppData\\Local\\Temp\\ipykernel_17564\\2689862664.py\", line 242, in objective_lgbm\n",
      "    model.fit(\n",
      "TypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "[W 2025-11-30 13:43:04,726] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 254\u001b[0m\n\u001b[0;32m    252\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    253\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[1;32m--> 254\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_lgbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest LGBM CV MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    261\u001b[0m ):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 242\u001b[0m, in \u001b[0;36mobjective_lgbm\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    224\u001b[0m     y_tr_k, y_val_k \u001b[38;5;241m=\u001b[39m y[tr_idx], y[val_idx]\n\u001b[0;32m    226\u001b[0m     model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\n\u001b[0;32m    227\u001b[0m         objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression_l1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    228\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         lambda_l2\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 242\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_tr_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     oof_pred[val_idx] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_k)\n\u001b[0;32m    250\u001b[0m mae_cv \u001b[38;5;241m=\u001b[39m mean_absolute_error(y, oof_pred)\n",
      "\u001b[1;31mTypeError\u001b[0m: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y = train[\"Tm\"].values\n",
    "smiles_train = train[\"SMILES\"].tolist()\n",
    "smiles_test = test[\"SMILES\"].tolist()\n",
    "# Liste aller RDKit-Deskriptoren\n",
    "all_desc = Descriptors._descList\n",
    "desc_names = [d[0] for d in all_desc]\n",
    "desc_funcs = [d[1] for d in all_desc]\n",
    "\n",
    "len(desc_names), desc_names[:10]\n",
    "def featurize_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        # Fallback: 0-Features\n",
    "        return np.zeros(len(desc_names) + 2048, dtype=float)\n",
    "\n",
    "    # RDKit Deskriptoren\n",
    "    desc_values = []\n",
    "    for f in desc_funcs:\n",
    "        try:\n",
    "            v = f(mol)\n",
    "        except Exception:\n",
    "            v = np.nan\n",
    "        desc_values.append(v)\n",
    "\n",
    "    desc_values = np.array(desc_values, dtype=float)\n",
    "\n",
    "    # Missing durch 0 ersetzen (oder median, je nach Geschmack)\n",
    "    desc_values = np.nan_to_num(desc_values, nan=0.0)\n",
    "\n",
    "    # Morgan Fingerprint\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "    fp_arr = np.array(fp, dtype=float)\n",
    "\n",
    "    return np.concatenate([desc_values, fp_arr])\n",
    "X_train = np.vstack([featurize_mol(s) for s in smiles_train])\n",
    "X_test  = np.vstack([featurize_mol(s) for s in smiles_test])\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "def cv_model(model_name, model_builder, X, y, X_test, n_splits=5, random_state=42):\n",
    "    from xgboost.callback import EarlyStopping as XgbEarlyStopping\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof_pred = np.zeros(len(y))\n",
    "    test_pred_folds = []\n",
    "    models = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n===== {model_name} | Fold {fold} / {n_splits} =====\")\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = model_builder()\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # LIGHTGBM (new API â€“ ONLY LightGBM callbacks)\n",
    "        # ---------------------------------------------------------\n",
    "        if model_name == \"LightGBM\":\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(50),\n",
    "                    lgb.log_evaluation(period=0)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # XGBOOST (new API â€“ ONLY XGBoost callbacks)\n",
    "        # ---------------------------------------------------------\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # CATBOOST (native early stopping)\n",
    "        # ---------------------------------------------------------\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                use_best_model=True,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type!\")\n",
    "\n",
    "        # prediction\n",
    "        val_pred = model.predict(X_val)\n",
    "        fold_mae = mean_absolute_error(y_val, val_pred)\n",
    "        print(f\"{model_name} Fold {fold} MAE: {fold_mae:.4f}\")\n",
    "\n",
    "        oof_pred[val_idx] = val_pred\n",
    "        test_pred_folds.append(model.predict(X_test))\n",
    "        models.append(model)\n",
    "\n",
    "    # final results\n",
    "    oof_mae = mean_absolute_error(y, oof_pred)\n",
    "    print(f\"\\n>>> {model_name} OOF MAE: {oof_mae:.4f}\")\n",
    "\n",
    "    return models, oof_pred, np.mean(test_pred_folds, axis=0), oof_mae\n",
    "\n",
    "\n",
    "\n",
    "def build_lgbm():\n",
    "    params = {\n",
    "        \"objective\": \"regression_l1\",\n",
    "        \"metric\": \"l1\",\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 48,\n",
    "        \"max_depth\": -1,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 3,\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"lambda_l1\": 1.0,\n",
    "        \"lambda_l2\": 1.0,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": 2000,\n",
    "    }\n",
    "    return lgb.LGBMRegressor(**params)\n",
    "\n",
    "def build_xgb():\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"max_depth\": 7,\n",
    "        \"subsample\": 0.85,\n",
    "        \"colsample_bytree\": 0.85,\n",
    "        \"n_estimators\": 1500,\n",
    "        \"reg_alpha\": 0.3,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"gamma\": 0.0,\n",
    "    }\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "def build_cat():\n",
    "    return CatBoostRegressor(\n",
    "        loss_function=\"MAE\",\n",
    "        learning_rate=0.03,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=5.0,\n",
    "        iterations=1500,\n",
    "        border_count=128,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "models_lgbm, oof_lgbm, test_lgbm, mae_lgbm = cv_model(\"LightGBM\", build_lgbm, X_train, y, X_test, n_splits=5)\n",
    "models_xgb, oof_xgb, test_xgb, mae_xgb = cv_model(\"XGBoost\", build_xgb, X_train, y, X_test, n_splits=5)\n",
    "models_cat, oof_cat, test_cat, mae_cat = cv_model(\"CatBoost\", build_cat, X_train, y, X_test, n_splits=5)\n",
    "oof_ens_simple = (oof_lgbm + oof_xgb + oof_cat) / 3.0\n",
    "mae_ens_simple = mean_absolute_error(y, oof_ens_simple)\n",
    "print(\"\\nEnsemble (simple average) OOF MAE:\", mae_ens_simple)\n",
    "oof_ens_weighted = 0.4 * oof_lgbm + 0.3 * oof_xgb + 0.3 * oof_cat\n",
    "mae_ens_weighted = mean_absolute_error(y, oof_ens_weighted)\n",
    "print(\"Ensemble (weighted) OOF MAE:\", mae_ens_weighted)\n",
    "test_ens_simple = (test_lgbm + test_xgb + test_cat) / 3.0\n",
    "test_ens_weighted = 0.4 * test_lgbm + 0.3 * test_xgb + 0.3 * test_cat\n",
    "sample = pd.read_csv(\"Submissions/sample_submission.csv\")\n",
    "\n",
    "# Einzelmodelle\n",
    "sub_lgbm = sample.copy()\n",
    "sub_lgbm[\"Tm\"] = test_lgbm\n",
    "sub_lgbm.to_csv(\"Submissions/submission_lgbm_rdkit.csv\", index=False)\n",
    "\n",
    "sub_xgb = sample.copy()\n",
    "sub_xgb[\"Tm\"] = test_xgb\n",
    "sub_xgb.to_csv(\"Submissions/submission_xgb_rdkit.csv\", index=False)\n",
    "\n",
    "sub_cat = sample.copy()\n",
    "sub_cat[\"Tm\"] = test_cat\n",
    "sub_cat.to_csv(\"Submissions/submission_cat_rdkit.csv\", index=False)\n",
    "\n",
    "# Ensembles\n",
    "sub_ens_simple = sample.copy()\n",
    "sub_ens_simple[\"Tm\"] = test_ens_simple\n",
    "sub_ens_simple.to_csv(\"Submissions/submission_ens_simple_rdkit.csv\", index=False)\n",
    "\n",
    "sub_ens_weighted = sample.copy()\n",
    "sub_ens_weighted[\"Tm\"] = test_ens_weighted\n",
    "sub_ens_weighted.to_csv(\"Submissions/submission_ens_weighted_rdkit.csv\", index=False)\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"lr\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\":    trial.suggest_int(\"leaves\", 24, 96),\n",
    "        \"max_depth\":     trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"feature_fraction\": trial.suggest_float(\"ff\", 0.7, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bf\", 0.7, 1.0),\n",
    "        \"bagging_freq\":     trial.suggest_int(\"bfreq\", 1, 7),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"minleaf\", 20, 150),\n",
    "        \"lambda_l1\":        trial.suggest_float(\"l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\":        trial.suggest_float(\"l2\", 1e-8, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_pred = np.zeros(len(y))\n",
    "\n",
    "    for tr_idx, val_idx in kf.split(X_train, y):\n",
    "        X_tr_k, X_val_k = X_train[tr_idx], X_train[val_idx]\n",
    "        y_tr_k, y_val_k = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective=\"regression_l1\",\n",
    "            metric=\"l1\",\n",
    "            n_estimators=2000,\n",
    "            verbosity=-1,\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            num_leaves=params[\"num_leaves\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            feature_fraction=params[\"feature_fraction\"],\n",
    "            bagging_fraction=params[\"bagging_fraction\"],\n",
    "            bagging_freq=params[\"bagging_freq\"],\n",
    "            min_data_in_leaf=params[\"min_data_in_leaf\"],\n",
    "            lambda_l1=params[\"lambda_l1\"],\n",
    "            lambda_l2=params[\"lambda_l2\"],\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_tr_k, y_tr_k,\n",
    "            eval_set=[(X_val_k, y_val_k)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False,\n",
    "        )\n",
    "        oof_pred[val_idx] = model.predict(X_val_k)\n",
    "\n",
    "    mae_cv = mean_absolute_error(y, oof_pred)\n",
    "    return mae_cv\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective_lgbm, timeout=60)\n",
    "\n",
    "print(\"Best LGBM CV MAE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc198a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Stacking train: (2662, 3)\n",
      "Shape Stacking test: (666, 3)\n",
      "Ridge Stacking Fold 1 MAE: 28.150086167260266\n",
      "Ridge Stacking Fold 2 MAE: 26.9482157070548\n",
      "Ridge Stacking Fold 3 MAE: 27.683435511399264\n",
      "Ridge Stacking Fold 4 MAE: 27.840998889533953\n",
      "Ridge Stacking Fold 5 MAE: 26.696597215815924\n",
      "\n",
      "Ridge Stacking OOF MAE: 27.463930773523842\n",
      "Saved submission_stack_ridge.csv\n",
      "Lasso Stacking Fold 1 MAE: 28.150085346317884\n",
      "Lasso Stacking Fold 2 MAE: 26.948214474626138\n",
      "Lasso Stacking Fold 3 MAE: 27.683438186931703\n",
      "Lasso Stacking Fold 4 MAE: 27.84100063810828\n",
      "Lasso Stacking Fold 5 MAE: 26.696596212976058\n",
      "\n",
      "Lasso Stacking OOF MAE: 27.463931046126106\n",
      "Saved submission_stack_lasso.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LEVEL-2 STACKING (Meta-Model fÃ¼r beste Kaggle-Performance)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Level-1 OOF Features\n",
    "X_stack = np.vstack([oof_lgbm, oof_xgb, oof_cat]).T\n",
    "X_stack_test = np.vstack([test_lgbm, test_xgb, test_cat]).T\n",
    "\n",
    "print(\"Shape Stacking train:\", X_stack.shape)\n",
    "print(\"Shape Stacking test:\", X_stack_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Ridge (sehr stabil fÃ¼r Stacking)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_ridge = np.zeros(len(y))\n",
    "test_ridge_folds = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_stack), 1):\n",
    "    X_tr, X_val = X_stack[tr_idx], X_stack[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    model_ridge = Ridge(alpha=0.1)\n",
    "    model_ridge.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predictions\n",
    "    oof_ridge[val_idx] = model_ridge.predict(X_val)\n",
    "    test_ridge_folds.append(model_ridge.predict(X_stack_test))\n",
    "\n",
    "    print(f\"Ridge Stacking Fold {fold} MAE:\",\n",
    "          mean_absolute_error(y_val, oof_ridge[val_idx]))\n",
    "\n",
    "ridge_mae = mean_absolute_error(y, oof_ridge)\n",
    "print(\"\\nRidge Stacking OOF MAE:\", ridge_mae)\n",
    "\n",
    "test_ridge = np.mean(test_ridge_folds, axis=0)\n",
    "\n",
    "# Save stacking submission\n",
    "sub_stack_ridge = sample.copy()\n",
    "sub_stack_ridge[\"Tm\"] = test_ridge\n",
    "sub_stack_ridge.to_csv(\"Submissions/submission_stack_ridge.csv\", index=False)\n",
    "print(\"Saved submission_stack_ridge.csv\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OPTIONAL: 2) Lasso Stacking (tends to use fewer signals)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "oof_lasso = np.zeros(len(y))\n",
    "test_lasso_folds = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_stack), 1):\n",
    "    X_tr, X_val = X_stack[tr_idx], X_stack[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    model_lasso = Lasso(alpha=0.001)\n",
    "    model_lasso.fit(X_tr, y_tr)\n",
    "\n",
    "    oof_lasso[val_idx] = model_lasso.predict(X_val)\n",
    "    test_lasso_folds.append(model_lasso.predict(X_stack_test))\n",
    "\n",
    "    print(f\"Lasso Stacking Fold {fold} MAE:\",\n",
    "          mean_absolute_error(y_val, oof_lasso[val_idx]))\n",
    "\n",
    "lasso_mae = mean_absolute_error(y, oof_lasso)\n",
    "print(\"\\nLasso Stacking OOF MAE:\", lasso_mae)\n",
    "\n",
    "test_lasso = np.mean(test_lasso_folds, axis=0)\n",
    "\n",
    "sub_stack_lasso = sample.copy()\n",
    "sub_stack_lasso[\"Tm\"] = test_lasso\n",
    "sub_stack_lasso.to_csv(\"Submissions/submission_stack_lasso.csv\", index=False)\n",
    "print(\"Saved submission_stack_lasso.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7481699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Stacking Meta-Model MAE: 27.307076424565917\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# STACKING META-MODEL (A)\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "X_meta = np.vstack([oof_lgbm, oof_xgb, oof_cat]).T\n",
    "X_meta_test = np.vstack([test_lgbm, test_xgb, test_cat]).T\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "meta = Ridge(alpha=1.0)\n",
    "meta.fit(X_meta, y)\n",
    "\n",
    "meta_oof_pred = meta.predict(X_meta)\n",
    "meta_test_pred = meta.predict(X_meta_test)\n",
    "\n",
    "mae_meta = mean_absolute_error(y, meta_oof_pred)\n",
    "print(\"\\nðŸš€ Stacking Meta-Model MAE:\", mae_meta)\n",
    "\n",
    "# Save prediction\n",
    "sub_stack = sample.copy()\n",
    "sub_stack[\"Tm\"] = meta_test_pred\n",
    "sub_stack.to_csv(\"Submissions/submission_stacking_ridge_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e9464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBOOST ONLY | Fold 1 / 5 =====\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "#     XGBOOST ONLY â€“ CLEAN STRONG BASELINE MODEL\n",
    "# ======================================================\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def cv_xgb_only(X, y, X_test, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_pred = np.zeros(len(y))\n",
    "    test_pred_folds = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f\"\\n===== XGBOOST ONLY | Fold {fold} / {n_splits} =====\")\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            learning_rate=0.03,\n",
    "            max_depth=6,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            n_estimators=10000,\n",
    "            reg_alpha=0.3,\n",
    "            reg_lambda=1.0,\n",
    "            tree_method=\"hist\",\n",
    "            gamma=0.0,)\n",
    "\n",
    "        # IMPORTANT: no early stopping for XGB >=2.0 sklearn API\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        pred_val = model.predict(X_val)\n",
    "        fold_mae = mean_absolute_error(y_val, pred_val)\n",
    "        print(f\"Fold MAE: {fold_mae:.4f}\")\n",
    "        display(Audio(\"C:/Windows/Media/Windows Ding.wav\", autoplay=True))\n",
    "        oof_pred[val_idx] = pred_val\n",
    "        test_pred_folds.append(model.predict(X_test))\n",
    "\n",
    "    oof_mae = mean_absolute_error(y, oof_pred)\n",
    "    print(\"\\n>>> XGB ONLY OOF MAE:\", oof_mae)\n",
    "\n",
    "    test_pred_mean = np.mean(test_pred_folds, axis=0)\n",
    "    return oof_pred, test_pred_mean, oof_mae\n",
    "\n",
    "\n",
    "# Run XGB baseline\n",
    "oof_xgb_solo, test_xgb_solo, mae_xgb_solo = cv_xgb_only(X_train, y, X_test)\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "#     STACKING: XGB (Level-1) + Ridge (Level-2)\n",
    "# ======================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Level-1: XGB OOF (from cv_xgb_only)\n",
    "X_stack = oof_xgb_solo.reshape(-1, 1)\n",
    "X_stack_test = test_xgb_solo.reshape(-1, 1)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = np.zeros(len(y))\n",
    "test_meta_folds = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_stack), 1):\n",
    "    print(f\"STACKING Fold {fold}/5\")\n",
    "\n",
    "    X_tr, X_val = X_stack[tr_idx], X_stack[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    meta = Ridge(alpha=0.1)\n",
    "    meta.fit(X_tr, y_tr)\n",
    "\n",
    "    oof_meta[val_idx] = meta.predict(X_val)\n",
    "    test_meta_folds.append(meta.predict(X_stack_test))\n",
    "\n",
    "meta_mae = mean_absolute_error(y, oof_meta)\n",
    "test_meta_pred = np.mean(test_meta_folds, axis=0)\n",
    "\n",
    "print(\"\\n>>> STACKING XGB+Ridge OOF MAE:\", meta_mae)\n",
    "\n",
    "# SUBMISSION: XGB ONLY\n",
    "sub_xgb_solo = sample.copy()\n",
    "sub_xgb_solo[\"Tm\"] = test_xgb_solo\n",
    "sub_xgb_solo.to_csv(\"Submissions/submission_xgb_only.csv\", index=False)\n",
    "\n",
    "# SUBMISSION: XGB + Ridge Stacking\n",
    "sub_xgb_stack = sample.copy()\n",
    "sub_xgb_stack[\"Tm\"] = test_meta_pred\n",
    "sub_xgb_stack.to_csv(\"Submissions/submission_xgb_stacked.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
