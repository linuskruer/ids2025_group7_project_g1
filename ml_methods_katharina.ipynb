{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc778abf",
   "metadata": {},
   "source": [
    "# Machine learning with random forest and SVM\n",
    "\n",
    "## 1.) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d5bb86-8a08-4dea-8a2e-ee37ea97f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required packages\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "#import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8861f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                       SMILES      Tm  Group 1  Group 2  Group 3  \\\n",
      "0  2175        FC1=C(F)C(F)(F)C1(F)F  213.15        0        0        0   \n",
      "1  1222  c1ccc2c(c1)ccc3Nc4ccccc4c23  407.15        0        0        0   \n",
      "2  2994          CCN1C(C)=Nc2ccccc12  324.15        2        1        0   \n",
      "3  1704                   CC#CC(=O)O  351.15        1        0        0   \n",
      "4  2526                    CCCCC(S)C  126.15        2        3        0   \n",
      "\n",
      "   Group 4  Group 5  Group 6  Group 7  ...  Group 415  Group 416  Group 417  \\\n",
      "0        0        0        0        0  ...          0          0          0   \n",
      "1        0        0        0        0  ...          0          0          0   \n",
      "2        0        0        0        0  ...          0          0          0   \n",
      "3        0        0        0        0  ...          0          0          0   \n",
      "4        0        0        0        0  ...          0          0          0   \n",
      "\n",
      "   Group 418  Group 419  Group 420  Group 421  Group 422  Group 423  Group 424  \n",
      "0          0          0          0          0          0          0          0  \n",
      "1          0          0          0          0          0          0          0  \n",
      "2          0          0          0          0          0          0          0  \n",
      "3          0          0          0          0          0          0          0  \n",
      "4          0          0          0          0          0          0          0  \n",
      "\n",
      "[5 rows x 427 columns]\n"
     ]
    }
   ],
   "source": [
    "## import datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "print(train.head())\n",
    "\n",
    "## split features and target variable\n",
    "X_train = train.drop([\"id\", \"SMILES\", \"Tm\"], axis = 1)\n",
    "y_train = train[\"Tm\"]\n",
    "X_test = test.drop([\"id\", \"SMILES\"], axis = 1) \n",
    "id_test = test[\"id\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a41cf-7967-40ea-a625-02b536cc3f8c",
   "metadata": {},
   "source": [
    "### 1.1.) RandomizedSearchCV\n",
    "\n",
    "- defining a range/distribution of values for each hyperparameter --> random sampling of a fixed number of combinations\n",
    "- faster than Grid Search; good for a first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d42099-58d6-491b-b503-4edaca1d3bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n",
      "Best parameters found:  {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.2, 'max_depth': 30}\n",
      "Best score (Neg. MAE):  -37.117318102559906\n"
     ]
    }
   ],
   "source": [
    "## parameter grid to sample from\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [100, 200, 400, 600, 800, 1000],\n",
    "    \"max_depth\": [10, 15, 20, 25, 30, 35, None],\n",
    "    \"min_samples_split\": [2, 3, 5, 7, 10],\n",
    "    \"min_samples_leaf\": [1, 3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.02, 0.05, 0.1, 0.2]} # percentages, e.g. 0.05 equals 5 % of features\n",
    "\n",
    "## Model initialization\n",
    "rf = RandomForestRegressor(random_state = 123)\n",
    "\n",
    "## RandomizedSearchCV initialization\n",
    "rf_randomized_search = RandomizedSearchCV(\n",
    "    estimator = rf,\n",
    "    param_distributions = parameters_grid,\n",
    "    n_iter = 70,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    scoring = \"neg_mean_absolute_error\",\n",
    "    random_state = 123,\n",
    "    error_score = \"raise\")\n",
    "\n",
    "## RandomizedSearchCV fit to the training data\n",
    "rf_randomized_search.fit(X_train, y_train)\n",
    "\n",
    "## Extraction of best result\n",
    "print(\"Best parameters found: \", rf_randomized_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", rf_randomized_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3305962-dff7-4178-a3c6-a248d3c4b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with found best parameters from randomized search\n",
    "\n",
    "rf1 = RandomForestRegressor(n_estimators = 400, min_samples_split = 2, min_samples_leaf = 1, max_features = 0.2, max_depth = 30, \n",
    "                            random_state = 123).fit(X_train, y_train)\n",
    "y_pred = rf1.predict(X_test)\n",
    "results_rf1 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_rf1.index.name = \"id\"\n",
    "results_rf1.to_csv(\"prediction_rf1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8be2e3-001f-42c2-aa44-aae843dafd2f",
   "metadata": {},
   "source": [
    "### 1.2.) GridSearchCV\n",
    "\n",
    "- every single combination of specified hyperparameters tested --> best combination within the defined grid found\n",
    "- can be very slow for large grids\n",
    "- good to use for fine-tuning after narrowing down via Random Search (done here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ec96fb-c285-44aa-852f-f80d3573b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'max_depth': None, 'max_features': 0.3, 'n_estimators': 600}\n",
      "Best score (Neg. MAE):  -35.96683474779305\n"
     ]
    }
   ],
   "source": [
    "## grid search 1\n",
    "\n",
    "# grid with parameters to search (for fine-tuning of n_estimators, max_depth and max_features\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [400, 600],\n",
    "    \"max_depth\": [20, 30, 40, None],\n",
    "    \"max_features\": [0.1, 0.15, 0.2, 0.25, 0.3]}\n",
    "\n",
    "# model initialisation\n",
    "rf = RandomForestRegressor(min_samples_split = 2, min_samples_leaf = 1, random_state = 123)\n",
    "\n",
    "# GridSearchCV initialisation\n",
    "rf_grid_search = GridSearchCV(estimator = rf, param_grid = parameters_grid, cv = 5, scoring = \"neg_mean_absolute_error\", n_jobs = -1,\n",
    "                              error_score = \"raise\")\n",
    "\n",
    "# GridSearchCV fit to the training data\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "## Extraction of best result\n",
    "print(\"Best parameters found: \", rf_grid_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8da3cfb-a9d9-4e16-ab97-d3a88e6a7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with found best parameters from grid search 1\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators = 600, min_samples_split = 2, min_samples_leaf = 1, max_features = 0.3, max_depth = None, \n",
    "                            random_state = 123).fit(X_train, y_train)\n",
    "y_pred = rf2.predict(X_test)\n",
    "results_rf2 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_rf2.index.name = \"id\"\n",
    "results_rf2.to_csv(\"prediction_rf2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fded397c-246c-4aa0-8d8c-fa60d789de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'max_depth': 35, 'max_features': 0.25, 'n_estimators': 400}\n",
      "Best score (Neg. MAE):  -36.463945340580196\n"
     ]
    }
   ],
   "source": [
    "## grid search 2\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_estimators\": [350, 400, 450, 500],\n",
    "    \"max_depth\": [25, 30, 35],\n",
    "    \"max_features\": [0.15, 0.2, 0.25]}\n",
    "\n",
    "rf = RandomForestRegressor(min_samples_split = 2, min_samples_leaf = 1, random_state = 123)\n",
    "\n",
    "rf_grid_search = GridSearchCV(estimator = rf, param_grid = parameters_grid, cv = 5, scoring = \"neg_mean_absolute_error\", n_jobs = -1,\n",
    "                              error_score = \"raise\")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", rf_grid_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "428424a0-d550-4fdd-90e7-516e72816c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with found best parameters from grid search 2\n",
    "\n",
    "rf3 = RandomForestRegressor(n_estimators = 400, min_samples_split = 2, min_samples_leaf = 1, max_features = 0.25, max_depth = 35, \n",
    "                            random_state = 123).fit(X_train, y_train)\n",
    "y_pred = rf3.predict(X_test)\n",
    "results_rf3 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_rf3.index.name = \"id\"\n",
    "results_rf3.to_csv(\"prediction_rf3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53412ea6-19b8-4b0d-9773-37d69de2315f",
   "metadata": {},
   "source": [
    "## 1.3) Optuna\n",
    "\n",
    "employs intelligent, adaptive sampling techniques like Bayesian optimization to guide the search, allowing it to converge on better results faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8603c29-53db-48ba-bfb6-bda02af9fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Objective Function\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 700)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, None)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.25, 0.3, 0.5, 0.7])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    # Initialize the Model\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        max_depth = max_depth,\n",
    "        min_samples_split = min_samples_split,\n",
    "        max_features = max_features,\n",
    "        criterion = criterion,\n",
    "        random_state = 123,\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    # Evaluate the Model using Cross-Validation\n",
    "    # Use 5-fold cross-validation on the training set\n",
    "    # 'accuracy' is the scoring metric. We negate the result because Optuna \n",
    "    # is often set to 'minimize', and minimizing negative accuracy is the same \n",
    "    # as maximizing positive accuracy.\n",
    "    score = cross_val_score(\n",
    "        clf, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        n_jobs=-1, \n",
    "        cv=5, \n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Return the mean cross-validation score (Accuracy)\n",
    "    # We return the NEGATED mean accuracy if direction is 'minimize'\n",
    "    # Since we use direction='maximize' later, we return the POSITIVE mean accuracy\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "# --- 3. Create and Run the Study ---\n",
    "\n",
    "# 3.1. Create the Study\n",
    "# We set the direction to 'maximize' because we want to maximize the mean accuracy.\n",
    "study = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=42) # TPE sampler is the default and best for many tasks\n",
    ")\n",
    "\n",
    "# 3.2. Run Optimization\n",
    "# The study will run the objective function 100 times, each time suggesting a new \n",
    "# set of parameters based on the results of the previous trials.\n",
    "print(\"Starting optimization...\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(\"Optimization finished.\")\n",
    "\n",
    "\n",
    "# --- 4. Analyze Results ---\n",
    "\n",
    "# Get the best trial information\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"\\n--- Best Trial Results ---\")\n",
    "print(f\"Best cross-validation accuracy: {best_trial.value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# --- 5. Final Model Training and Evaluation (Optional) ---\n",
    "# Train the final model with the best parameters on the entire training set\n",
    "final_clf = RandomForestClassifier(**best_trial.params, random_state=42)\n",
    "final_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the unseen test set\n",
    "test_accuracy = final_clf.score(X_test, y_test)\n",
    "print(f\"\\nTest set accuracy with optimal parameters: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e16ad0-1653-40f9-91c6-1dc4a5d8d51c",
   "metadata": {},
   "source": [
    "## 2.) SVR with radial kernel\n",
    "\n",
    "SVR: a tube with an estimated function (hyperplane) in the middle and boundaries on either side defined by ε. The algorithm’s goal is to minimize the error by identifying a function that puts more of the original points inside the tube while at the same time reducing the \"slack.\"\n",
    "Difference to SVM: the support vectors are the points that fall outside the tube rather than the ones at the margin\n",
    "\n",
    "Radial and polynomial kernel: A kernel is a function that takes the original non-linear problem and transforms it into a linear one, which is then handled by the algorithm in a higher-dimensional space.\n",
    "\n",
    "Hyperparameters that need to be tuned:\n",
    "- C (regularization parameter): high C leads to stronger penalization of errors (points outside the tube), leading to a more complex model that fits the data more closely; small C allows more errors and leads to a simpler model with a larger margin\n",
    "- epsilon: value of epsilon determines the width of the tube around the estimated function (hyperplane); points that fall inside this tube are considered as correct predictions and are not penalized\n",
    "- gamma: inverse of the radius of influence of samples selected by the model as support vectors; with a low gamma the influence of individual training examples reaches further, affecting a larger region of the feature space (leads to a smoother and less complex decision boundary, but can result in underfitting); with a high gamma the influence is closer, affecting only the region near the training example (can lead to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b368b11-9929-4363-ba0d-dd14fb4f01ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'gamma': 'scale', 'epsilon': 0.01, 'C': 1000}\n",
      "Best score (Neg. MAE):  -32.52189746601051\n"
     ]
    }
   ],
   "source": [
    "## randomized search\n",
    "\n",
    "parameters_grid = {\n",
    "    \"C\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 1000],\n",
    "    \"epsilon\": [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 4],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.01, 1, 10]}\n",
    "\n",
    "svr_rbf = SVR(kernel = \"rbf\")\n",
    "\n",
    "svr_rbf_randomized_search = RandomizedSearchCV(\n",
    "    estimator = svr_rbf,\n",
    "    param_distributions = parameters_grid,\n",
    "    n_iter = 70,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    scoring = \"neg_mean_absolute_error\",\n",
    "    random_state = 123,\n",
    "    error_score = \"raise\")\n",
    "\n",
    "svr_rbf_randomized_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", svr_rbf_randomized_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", svr_rbf_randomized_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f1f3d6-1991-4a48-8caf-73a03e621abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_1 = SVR(kernel = \"rbf\", gamma = \"scale\", epsilon = 0.01, C = 1000).fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr_rbf_1.predict(X_test)\n",
    "results_svr_rbf_1 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_svr_rbf_1.index.name = \"id\"\n",
    "results_svr_rbf_1.to_csv(\"prediction_svr_rbf_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d43fe51-0557-4a67-8f42-7c6ef6a61d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023719825417948447\n"
     ]
    }
   ],
   "source": [
    "print(svr_rbf_1._gamma)\n",
    "# gamma = \"scale\" uses this value for gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d60d2d-4c7f-4326-8562-1207fda1d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 1000, 'epsilon': 0.5, 'gamma': 'scale'}\n",
      "Best score (Neg. MAE):  -32.48013996312687\n"
     ]
    }
   ],
   "source": [
    "## grid search\n",
    "\n",
    "parameters_grid = {\n",
    "    \"C\": [0.1, 0.5, 1, 10, 100, 500, 1000, 2000],\n",
    "    \"epsilon\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "svr_rbf = SVR(kernel = \"rbf\")\n",
    "\n",
    "svr_rbf_grid_search = GridSearchCV(\n",
    "    estimator = svr_rbf,\n",
    "    param_grid = parameters_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    scoring = \"neg_mean_absolute_error\",\n",
    "    error_score = \"raise\")\n",
    "\n",
    "svr_rbf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", svr_rbf_grid_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", svr_rbf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a345e1e1-7dc9-44f6-a5ec-f5eb9ce13ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf_2 = SVR(kernel = \"rbf\", gamma = \"scale\", epsilon = 0.5, C = 1000).fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr_rbf_2.predict(X_test)\n",
    "results_svr_rbf_2 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_svr_rbf_2.index.name = \"id\"\n",
    "results_svr_rbf_2.to_csv(\"prediction_svr_rbf_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad287da-b242-4961-9ea6-0d3d896f640f",
   "metadata": {},
   "source": [
    "## 3.) SVM with polynomial kernel\n",
    "\n",
    "Hyperparameters that need to be tuned:\n",
    "- C (see above)\n",
    "- epsilon (see above)\n",
    "- d (degree): degree of the polynomial function; determines the dimensionality of the feature space transformation; higher d --> more complex, non-linear relationships can be captured, but computation time and risk of overfitting increases; d=1 is linear kernel, d=2 is quadratic, d=3 is cubic\n",
    "- gamma: see above; often left at default value or searched over a small space\n",
    "- coef0 (r): independent term in polynomial kernel; shifts the input data; allows the kernel to map the data non-linearly even if d=1; often less crucial than C and d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902ad920-0eef-4fe2-843d-ba1a25e2878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 100, 'coef0': 1, 'degree': 3, 'epsilon': 0.1, 'gamma': 'scale'}\n",
      "Best score (Neg. MAE):  -36.46079321797002\n"
     ]
    }
   ],
   "source": [
    "## grid search (directly done and with smaller grid and lower cv as svr with polynomial kernel very computationally heavy - took forever before)\n",
    "\n",
    "parameters_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"epsilon\": [0.001, 0.01, 0.1],\n",
    "    \"degree\": [2, 3],\n",
    "    \"gamma\": [\"scale\", 0.001, 0.01],\n",
    "    \"coef0\": [-1, 0, 1]} \n",
    "\n",
    "svr_poly = SVR(kernel = \"poly\")\n",
    "\n",
    "svr_poly_grid_search = GridSearchCV(\n",
    "    estimator = svr_poly,\n",
    "    param_grid = parameters_grid,\n",
    "    cv = 4,\n",
    "    n_jobs = -1,\n",
    "    scoring = \"neg_mean_absolute_error\",\n",
    "    error_score = \"raise\")\n",
    "\n",
    "svr_poly_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", svr_poly_grid_search.best_params_)\n",
    "print(\"Best score (Neg. MAE): \", svr_poly_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9221385-6b25-41fa-8ec2-5d4f09f9107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_poly_1 = SVR(kernel = \"poly\", gamma = \"scale\", epsilon = 0.1, C = 100, degree = 3, coef0 = 1).fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr_poly_1.predict(X_test)\n",
    "results_svr_poly_1 = pd.DataFrame({\"id\": id_test, \"Tm\": y_pred})\n",
    "results_svr_poly_1.index.name = \"id\"\n",
    "results_svr_poly_1.to_csv(\"Submissions/prediction_svr_poly_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44472aba-7e3e-422a-ad7c-d5ec45ad4b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d67439-b772-4943-b867-edf662817bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3c159-5953-4136-b06a-a6fb41df43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do everything with Optuna too\n",
    "## measures to prevent overfitting?!\n",
    "## standardization/normalization of the data?\n",
    "## also try SVR with other kernels: ‘linear’ (if a linear relationship is suspected),‘sigmoid’ (also for non-linear), ‘precomputed’"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
